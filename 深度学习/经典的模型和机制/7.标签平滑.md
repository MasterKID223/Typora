## 标签平滑

参考：[邱锡鹏 p191 7.7.6 标签平滑]

代码：[[github](https://github.com/CoinCheung/pytorch-loss)]

### 原理

在数据增强中，我们可以给样本特征加入随机噪声来避免过拟合。同样，我们也可以给样本的标签引入一定的噪声。假设训练数据集中有一些样本的标签是被错误标注的，那么最小化这些样本上的损失函数会导致过拟合。一种改善的正则化方法是**标签平滑（Label Smoothing）**，即在输出标签中添加噪声来避免模型过拟合。

一个样本 $x$ 的标签可以用one-hot向量表示，即
$$
y = [0, \ldots, 0, 1, 0, 0, \ldots, 0]^T
$$
这种标签可以看作**硬目标（Hard Target）**。如果使用Softmax分类器并使用交叉熵损失函数，最小化损失函数会使得正确类和其他类的权重差异变得很大。根据Softmax函数的性质可知，如果要使得某一类的输出概率接近于1，其未归一化的得分要远大于其他类的得分，可能会导致其权重越来越大，并导致过拟合。此外，如果样本标签是错误的，会导致更严重的过拟合现象。为了改善这种情况，我们可以引入一个噪声对标签进行平滑，即假设样本以 $\epsilon$ 的概率为其他类。平滑后的标签为：
$$
\tilde{y} = [\frac{\epsilon}{K-1}, \ldots, \frac{\epsilon}{K-1}, 1-\epsilon, \frac{\epsilon}{K-1}, \ldots, \frac{\epsilon}{K-1}]^T
$$
其中 $K$ 为标签数量，这种标签可以看作**软目标（Soft Target）**。标签平滑可以避免模型的输出过拟合到硬目标上，并且通常不会损害其分类能力。

上面的标签平滑方法是给其他 $K-1$ 个标签相同的概率 $\frac{\epsilon}{K-1}$ ，没有考虑标签之外的相关性。一种更好的做法是按照类别相关性来赋予其他标签不同的概率。比如先训练另外一个更复杂（一般为多个网络的集成）的**教师网络（Teacher Network）**，并使用大网络的输出作为软目标来训练**学生网络（Student Network）**这种方法也称为**知识蒸馏（Knowledge Distillation）**。

### 代码

[[github](https://github.com/CoinCheung/pytorch-loss/blob/master/pytorch_loss/label_smooth.py#L14)]