# NCE 到 InfoNCE loss

[Noise Contrastive Estimation 前世今生——从 NCE 到 InfoNCE](https://zhuanlan.zhihu.com/p/334772391)



### 1 从NLP入手



#### 1.1 背景

NCE，也就是 [Noise Contrastive Estimation](http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf)（噪声对比估计）。论文中估计的是概率密度函数（pdf, probability density function）。而 NLP 中的 word 或 vision 中的 pixel 都是离散的，且我们感兴趣的是的概率质量函数（pmf, probability mass function），因此我主要参考了 [[4]](https://www.cs.toronto.edu/~amnih/papers/ncelm.pdf) 这篇论文。它就是在使用 NCE 时假设了离散分布，并用 pmf 代替其中 pdf，然后将 NCE 应用到 NLP 领域。

#### 1.2 n-gram

- **语言模型（language model，LM）**

  [语言模型](https://zhuanlan.zhihu.com/p/90741508)

  一个语言模型通常构建为字符串 $s$ 的概率分布 $p(s)$ ，这里 $p(s)$ 试图反映的是字符串 $s$ 作为一个句子出现的频率。

  对于一个由 $m$ 个基元（“基元”可以为字、词或短语等，为了表述方便，以后我们只用“词”来通指）构成的句子 $s=w_1,w_2,...,w_m$ ，其概率计算公式可以表示为
  $$
  p(s)=p(w_1)p(w_2|w_1)p(w_3|w_1,w_2)...p(w_m|w_1,...,w_{m−1})= \\
  ∏\limits_i^m = p(wi|w_1,...,w_{i-1})
  $$
  上式中， $p(w_i|w_1,w_2,⋯,w_{i−1})$ 表示产生第 $i(1≤i≤m)$ 个词的概率是由已经产生的 $i−1$ 个词 $w_1,w_2,⋯,w_{i−1}$ 决定的。如果能对这一项建模，那么只需把每个位置的条件概率相乘，就能计算出 $p(s)$ 。然而一般来说，这个参数量是巨大的，假设一门语言的词汇量为$V$，对于句子 $s=w_1,w_2,⋯,w_m$ ，所需参数数量为 $Vm$ 。

我们将条件概率的条件$(w_1,w_2,⋯,w_{i−1})$ 称为单词 $w_i$ 的上下文，用 $c_i$ 表示。
$$
p(s) = ∏\limits_i^m = p(w_i|c_i) \tag{1}
$$
可以看到，language model 就是条件概率  $p(w|c)$ 的集合，但是直接计算每个 $w$ 在语料库中的条件概率是需要很大计算量的。因此在统计语言模型中，引入了马尔可夫假设，即**“一个词出现的概率只与它前面出现的有限的一个或者 n 个词有关”**，将这 n 个词称为一个 gram，这就是著名的 n-gram 模型，因此可以将模型简化为：
$$
p(w_1,w_2,w_3,…,w_m)=∏\limits_{i=1}^m p(w_i∣w_{i−n+1},…,w_{i−1}) \tag{2}
$$




#### 1.3 最大似然估计



*概率是根据事物的性质判断将要发生的情况。似然是根据已经发生的情况，推测事物本身的性质。[最大似然估计的介绍](https://www.bilibili.com/video/BV1CR4y1L7RC)：利用已知的样本标记结果，反推最具有可能或者最大概率导致这些样本结果出现的模型参数。*

![image-20221017195248157](.pic/image-20221017195248157.png)

![image-20221017195734763](.pic/image-20221017195734763.png)

上面的 n-gram 构建语言模型的方法实际上就是，将一个训练语料库中的每个 $w_i$ 和它的 $c_i$ (也就是由前面n个 $w$ 构成)的条件概率计算出来并储存（实际操作上是统计每个gram出现的次数），然后下一次计算某个句子的出现的概率时，即 $(2)$ 式，就在存储中找到这个句子中出现的 $w$ 和 $c$ 的条件概率，然后乘起来即可。

因此，我们是否可以不事先计算并存储每个 $w$ 和  $c$  条件概率，而是建立一个模型(或者说函数)，给这个模型一组 $w$ 和  $c$  就能输出它们的条件概率。

在机器学习领域有一个方法是：对所要考虑的问题建模后为其构造一个目标函数，然后对这个目标函数进行优化，从而求得一组最优的参数，最后利用这组最优参数对应的模型进行预测，也就是**最大似然估计**。

[最大似然估计的一般求解步骤](https://baike.baidu.com/item/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/4967925#4)

在建模统计语言模型时，利用最大似然估计，根据 $(1)$ 式目标函数，我们可以写出其对数似然函数如下：
$$
\mathcal{L}_{MLE}=\sum\limits_{i=2}^m log⁡p_θ(w_i∣c_i) \tag{3}
$$
然后最大化对数似然函数 $\ell_{MLE}$，实际上这样就是将 $p(w|c)$ 看成 $w$ 和  $c$  的函数， $θ$ 为待定参数集： 
$$
p_θ(w|c)=F(w,c;θ) \tag{4}
$$
这样一旦最优参数集 $θ^∗$ 可以确定，函数 $F$ 就被唯一确定，那么对于任何概率 $p(w|c)$ 都可以用函数 $F(w,c;θ^∗)$ 来计算了。

<font color=red>**那么如何计算出  $θ^∗$ 呢？**</font>

根据上面最大似然估计的一般求解步骤的最后一步，等式两边都对 $θ$ 求导，然后令导数不断逼近于0，从而算出 $θ^∗$。



#### 1.4 神经概率语言模型

面的方法似然看起来很美好，但其中有两个问题：

- 如何构造一个好的函数 $F$ 。
- 最大似然估计虽然理论上简单可行，但对于某些模型，在实际计算时可能需要很大的计算量，因此未必容易。

首先来看第一个问题，这也就是我们为什么引入神经网络，因为神经网络理论上可以表示任何函数，那么通过训练，肯定能找到这个合适的 $F$ ，因此 Bengio 等人在 2003 年 A Neural Probabilistic Language Model [8] 中提出了神经概率语言模型（NPLM）。其不在受限于 gram 的大小，可以在包含任意大小上下文的情况下建模 $w$ 的条件概率。

具体来看，它把语言模型的建立当作一个多分类问题，我们用 $V={v_1,v_2,...,v_{|V|}}$ 表示一个包含所有单词的单词库，其大小为 $|V|$ ，将 $(w,c)$ 当成一对训练样本（实际上 $w$ 会转换成词向量，矩阵形式，这里不做详解），通**过神经网络后和 softmax 后**，输出一个向量 $\hat{y}=[\hat{y}_{i,1},\hat{y}_{i,2},...,\hat{y}_{i,|V|}]$ , 其中每一维 $\hat{y}_{i,j}=p(v_j|c_i)$ 表示上下文为 $ci$ 时 第 $i$ 个单词 $w_i$ 是单词库中第 $j$ 个单词 $v_j$ 的概率，训练过程要求最后单词库中概率最大的单词就是训练样本对中的 $w_i$ 。这样训练结束后，给神经网络一个上下文 $c_l=(w_1,w_2,...,w_{l−1})$ ，神经网络就能预测在当前上下文 $c_l$ 时，下一个 单词 $w_l$ 是单词库中的各个词的概率 $p(w_l|c_l)$ ，通过这个我们也就可以构建语言模型。

- softmax函数

  对于多类问题，类别标签 $y \in \{1,2,...,C\}$ 可以有 $C$ 个取值，给定一个样本 $x$ ，Softmax回归预测的属于类别 $c$ 的条件概率为，其中，$w_c$ 是第 $c$ 类的权重向量：
  $$
  p(y=c|x) = softmax(w_c^Tx)  = \dfrac{exp(w_c^Tx)}{\sum\limits_{c'=1}^C exp(w_{c'}^Tx)} \tag{邱锡鹏 p61}
  $$
  

假设输入到 softmax 前的结果用 $s_θ(w,c)$ 表示，实际上 $s_θ(w,c)$ 是有含义的，它是一个 socring function(**Attention机制，注意力打分函数**) ，**输出的分数用来量化** $w$ **在上下文** $c$ **中匹配性**（relation），那么 $w$ 条件概率(**注意力分布**)可以表示为以下形式： 
$$
p_θ(w|c)=\dfrac{exp(s_θ(w,c))}{∑\limits_{w'∈V}exp(s_θ(w',c))}= \dfrac{u_θ(w,c)}{Z(c)} \tag{5}
$$
式中， $u_θ(w,c)= exp(s_θ(w,c))$ 表示当前单词 $w$ 在单词库中的概率；令 $Z(c)=∑\limits_{w'∈V}exp(s_θ(w',c))$ 表示当前单词库中所有单词的概率的累和，通常将这一项叫做“配分函数”或“归一化因子”。一般来说，单词库 $|V|$ 的数量是非常巨大的，**因此计算** $Z(c)$ **是非常昂贵、耗时的一件事，这也就是 NCE 要解决的问题。（见附录1）**

如果我们不考虑 $s_θ(w,c)$ 的具体形式，那么 $(5)$ 式实际上就可以当作我们在 $(4)$ 式中所构造的函数 $F$ 的表达式， 既然如此，那我们接着用 1.3 中提到的最大似然估计的方式来试着求解 $F$ 的参数 $θ$ 。我们将从句子 $s$ 中取样的 $w$ 看成经验分布(数据分布，已知的) $\widetilde{p}(w|c)$ ， $(3)$ 式中的 $\mathcal{L}_{MLE}$ 可以写成（[最大期望算法](https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%E6%9C%80%E5%A4%A7%E6%9C%9F%E6%9C%9B%E7%AE%97%E6%B3%95/10180861%232)）：
$$
arg⁡max\mathcal{L}_{MLE} =arg⁡max∑_{w \sim \widetilde{p}(w|c)}log⁡p_θ(w∣c) \\
= arg⁡max \mathbb{E}_{w \sim \widetilde{p}(w|c)} \dfrac{log⁡u_θ(w,c)}{Z(c)} \tag{6}
$$
现在要最大化 $\mathcal{L}_{MLE}$ ，那么将其关于 $θ$ 求导：
$$
\dfrac{\partial\mathcal{L}_{MLE}}{\partial\theta}= \mathbb{E}_{w \sim \widetilde{p}(w|c)}\dfrac{\partial log\frac{\mu_\theta(w,c)}{Z(c)}}{\partial\theta} \\
=\mathbb{E}_{w \sim \widetilde{p}(w|c)}[{\dfrac{\partial log\mu_\theta(w,c)}{\partial\theta} - \dfrac{\partial Z(c)}{\partial\theta}}] \\
=\mathbb{E}_{w \sim \widetilde{p}(w|c)}{\dfrac{\partial log\mu_\theta(w,c)}{\partial\theta} - \dfrac{\partial Z(c)}{\partial\theta}} \\
\tag{7}
$$
这里解释一下上面到最后一步的转换，因为 $Z(c)=∑\limits_{w'∈V}exp(s_θ(w',c))$ ，其中 $w′$ 为单词库 $V$ 中所有的单词，而单词库其中每个单词的概率由 $p_θ(w|c)$ 产生，因此  $w′∼p_θ(w|c)$ ，与经验分布 $w∼\widetilde{p}(w|c)$ 不相关，所以可以把期望 $\mathbb{E}_{w \sim \widetilde{p}(w|c)}$ 去掉。

 $(7)$式结果中的 $\dfrac{∂log⁡Z(c)}{∂θ} $ 计算如下：
$$
\begin{aligned}
\frac{\partial}{\partial \theta} \log Z(c) &=\frac{1}{Z(c)} \frac{\partial}{\partial \theta} Z(c) \\
&=\frac{1}{Z(c)} \frac{\partial}{\partial \theta} \sum_{w^{\prime} \in V} u_\theta(w, c) \\
&=\frac{1}{Z(c)} \frac{\partial}{\partial \theta} \sum_{w^{\prime} \in V} \exp \left(s_\theta(w, c)\right) \\
&=\sum_{w^{\prime} \in V} \frac{1}{Z(c)} \exp \left(s_\theta(w, c)\right) \frac{\partial}{\partial \theta} s_\theta(w, c) \\
&=\sum_{w^{\prime} \in V} p_\theta(w \mid c) \frac{\partial}{\partial \theta} s_\theta(w, c) \\
&=\mathbb{E}_{w \sim p_\theta(w \mid c)} \frac{\partial}{\partial \theta} s_\theta(w, c) \\
&=\mathbb{E}_{w \sim p_\theta(w \mid c)} \frac{\partial}{\partial \theta} \log _\theta(w, c) \\
\end{aligned}
\tag{8}
$$
将 $(8)$ 式结果带回 $(7)$ 式中得：

$$
\begin{aligned}
\frac{\partial}{\partial \theta} \mathcal{L}_{\mathrm{MLE}} &=\mathbb{E}_{w \sim \tilde{p}(w \mid c)} \frac{\partial}{\partial \theta} \log u_\theta(w, c)-\frac{\partial}{\partial \theta} \log Z(c) \\
&=\mathbb{E}_{w \sim \tilde{p}(w \mid c)} \frac{\partial}{\partial \theta} \log u_\theta(w, c)-\mathbb{E}_{w \sim p_\theta(w \mid c)} \frac{\partial}{\partial \theta} \log u_\theta(w, c) \\
&=\sum_w \tilde{p}(w \mid c) \frac{\partial}{\partial \theta} \log u_\theta(w, c)-\sum_w p_\theta(w \mid c) \frac{\partial}{\partial \theta} \log _\theta(w, c) \\
&=\sum_w\left[\tilde{p}(w \mid c) \frac{\partial}{\partial \theta} \log u_\theta(w, c)-p_\theta(w \mid c) \frac{\partial}{\partial \theta} \log u_\theta(w, c)\right] \\
&=\sum_w\left[\left(\tilde{p}(w \mid c)-p_\theta(w \mid c)\right) \frac{\partial}{\partial \theta} \log u_\theta(w, c)\right]
\end{aligned}
\tag{9}
$$
最大似然好像很容易，但是实际上还是绕不开对“归一化常数$Z(c)$”的计算，所以就需要 NCE 登场了。



### 2 什么是NCE

上一节中说明了计算 $Z(c)$ 非常昂贵这个问题需要解决，一个简单的思路是将 $Z(c)$ 也看出模型的 一个参数 $z_c$ 来进行训练, 但是这种方法不适合于上面提到的最大似然估计, 因为由 (6) 式可以看 出来, 它会直接将 $z_c$ 趋于 0 来获得最大似然。因此，有人提利用这个思想提出了一些不定义 $Z(c)$, 直接用 $u_\theta(w, c)$ 估计模型的方法, 如 [contrastive divergence (Hinton, 2002)](https://link.zhihu.com/?target=https%3A//www.cs.toronto.edu/~hinton/absps/tr00-004.pdf)和 [score matching (Hyvarinen, 2005)](https://link.zhihu.com/?target=https%3A//jmlr.org/papers/volume6/hyvarinen05a/old.pdf)。（见附录2)

而 NCE 不同于上面两种方法, 它是通过最大化同一个目标函数来估计模型参数 $\theta$ 和归一化常数, NCE 的**核心思想就是通过学习数据分布样本和噪声分布样本之间的区别, 从而发现数据中的一些 特性**, 因为这个方法需要依靠与噪声数据进行对比, 所以称为 “噪声对比估计 (Noise Contrastive Estimation)"。更具体来说, **NCE 将问题转换成了一个二分类问题, 分类器能够对 数据样本和噪声样本进行二分类, 而这个分类器的参数 $\theta$ 就等价于 $1.4$ 中我们想要得到 $\theta$ 。**（见附录3)

现在假设一个特定上下文 $c$ 的数据分布为 $\tilde{p}(w \mid c)$, 我们称从它里面取出的样本为正样本, 令类别 $D=1$; 而另一个与 $c$ 无关的噪声分布为 $q(w)$, 我们称从里面取出的样本为负样本, 令类别为 $D=0$ 。遵循 [Gutmann and Hyvrinen (2012) [3]](https://link.zhihu.com/?target=https%3A//www.jmlr.org/papers/volume13/gutmann12a/gutmann12a.pdf) 中的设置, 假设现在取出了 $k_d$ 个正样本和 $k_n$ 个负样本，将这些正负样本混合形成一个混合分布 $p(w \mid c)$ 。

我们得到下面这些概率:
$$
\begin{array}{r}
p(D=1)=\frac{k_d}{k_d+k_n} \\
p(D=0)=\frac{k_n}{k_d+k_n} \\
p(w \mid D=1, c)=\tilde{p}(w \mid c) \\
p(w \mid D=0, c)=q(w)
\end{array}
\tag{10}
$$
所以可以计算后验概率(贝叶斯定理):
$$
\begin{aligned}
p(D=0 \mid w, c) &=\frac{p(D=0) p(w \mid D=0, c)}{p(D=0) p(w \mid D=0, c)+p(D=1) p(w \mid D=1, c)} \\
&=\frac{\frac{k_n}{k_d+k_n} \times q(w)}{\frac{k_d}{k_d+k_n} \times \tilde{p}(w \mid c)+\frac{k_n}{k_d+k_n} \times q(w)} \\
&=\frac{\frac{k_n}{k_d} \times q(w)}{\tilde{p}(w \mid c)+\frac{k_n}{k_d} \times q(w)} \\
p(D=1 \mid w, c) &=\frac{p(D=1) p(w \mid D=1, c)}{p(D=0) p(w \mid D=0, c)+p(D=1) p(w \mid D=1, c)} \\
&=\frac{\frac{k_d}{k_d+k_n} \times \tilde{p}(w \mid c)}{\frac{k_d}{k_d+k_n} \times \tilde{p}(w \mid c)+\frac{k_n}{k_d+k_n} \times q(w)} \\
&=\frac{\tilde{p}(w \mid c)}{\tilde{p}(w \mid c)+\frac{k_n}{k_d} \times q(w)}
\end{aligned}
\tag{11}
$$