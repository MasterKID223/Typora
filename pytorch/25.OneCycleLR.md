### [torch.optim.lr_scheduler.OneCycleLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.OneCycleLR.html#onecyclelr)



- 作用

  设置参数学习率的调度，使用1周期学习率策略，先从初始学习率调整到最大学习率，再从最大学习率下降到最小学习率，最小学习率比初始化学习率小的多。在训练过程中，1周期学习率调度策略每个batch都执行一次。

- 函数

  ```python
  CLASS torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, total_steps=None, epochs=None, steps_per_epoch=None, pct_start=0.3, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=25.0, final_div_factor=10000.0, three_phase=False, last_epoch=- 1, verbose=False)
  ```

  

- 例子

  ```shell
  >>> data_loader = torch.utils.data.DataLoader(...)
  >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)
  >>> scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(data_loader), epochs=10)
  >>> for epoch in range(10):
  >>>     for batch in data_loader:
  >>>         train_batch(...)
  >>>         scheduler.step()
  ```

  