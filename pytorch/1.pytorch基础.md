## pytorch加载数据

### Dataset类	

1. 抽象类，所有子类需要重写下面方法
2. `__getitem__()`方法：根据给定的键(key)返回数据样本
3. `__len__()`：返回数据集的长度/大小
4. 用来获取数据和其label，返回有多少数据

### Dataloader类

1. 实现Dataset类，打包数据集

1. 参数

   ```javascript
   dataset: 数据集路径
   batch_size: 一次加载多少数据样本(default:1)
   shuffle: 每次的epoch加载的数据是乱序的(default:False)
   sampler: 采样器，决定如何从数据集中读取数据样本，可以是任何实现`__len__`方法的Iterable对象
   batch_sampler: 返回一个batch数据的索引，和`batch_size`,`shuffle`,`sampler`属性冲突
   num_workers: 线程数
   collate_fn: callable类型，数据集是map类型时使用，用来把数据样本合并成一个mini-batch张量
   pin_memory: 如果True，在返回tensors之前data loader把tensors复制到cuda的固定内存中。如果数据元素是自定类型不进行此操作
   drop_last: 数据集的大小不能被batch size整除，就把最后一个batch删除(True)。否则最后一个batch会更小(default:False)
   timeout: 超时(default:0)
   worker_init_fn: callable类型，线程的初始化函数
   generator: torch.Generator类型，随机成成索引，并为多线程处理生成`base_seed`
   prefetch_factor: 值为int，'2'表示预先获取2*num_workers的数据样本
   persistent_workers: True在加载完dataset之后，不会杀死data loader进程，这保持了Dataset实例的活动状态
   
   ```
   

    ```python
   # usage
   test_loader = DataLoader(dataset=test_data, batch_size=4, shuffle=True, num_workers=0, drop_last=False)
   for data in test_loader:
       imgs, targets = data
       print(imgs.shape)
       print(targets)
       break
   '''
   output: 
   torch.Size([4, 3, 32, 32])
   tensor([2, 4, 4, 5])
   '''
    ```



## Tensorboard的使用

### SummaryWriter类

```json
log_dir: 日志目录
comment: 日志前缀（类似git comment）
```

1.  `add_scalar()`方法

    ```json
    往Tensorboard上添加坐标图
    tag
    scalar_value: 纵轴
    gloabl_step: 训练步骤
    ```

2. `add_image()`方法

    ```json
    往Tensorboard上添加图片
    添加多张图片使用`add_images()`
    tag
    img_tensor: 图片, torch.Tensor, numpy.array, string/blobname
    global_step: 训练步骤
    dataformats: (default: "CHW")
    ```




### 调用tensorboard

```json
tensorboard --logdir=logs --port=7777
```



## Transforms的使用

```python
from torchvision import transforms
```

1. `ToTensor(pic)`

   ```python
   # 把PIL、np.array类型的图片转换成tensor
   # usage:
   from torchvision import transforms
   from PIL import Image
   
   img_path = "data/train/ants_image/0013035.jpg"
   img = Image.open(img_path)
   # 创建ToTensor类
   trans_totensor = transforms.ToTensor()
   img_tensor = trans_totensor(img)
   ```

2. `Compose`

   ```python
   # 组合一些transforms
   # usage: 
   # Example:
   # __init__()
   transforms.Compose([
       ransforms.CenterCrop(10),
       transforms.PILToTensor(),
       transforms.ConvertImageDtype(torch.float),
   ])
   # __call__()
   trans_compose = transforms.Compose([trans_resize, trans_totensor])
   img_compose = trans_compose(img)
   ```

3. `Normalize`			

    ```python
    # 归一化
    # mean: 图像每个通道的均值, std: 标准差
    def __init__(self, mean, std, inplace=False)
    # 对图像tensor进行归一化
    def forward(self, tensor: Tensor) -> Tensor
    # usage
    trans_nomal = transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    img_noaml = trans_nomal.forward(img_tensor)
    ```
    
4. `Resize`

    ```python
    # 图像缩放
    # size: (h, w), 若输入是torch Tensor类型, 则size: [c, h, w]
    def __init__(self, size, interpolation=InterpolationMode.BILINEAR, max_size=None, antialias=None)
    # 调用
    def forward(self, img)
    # usage
    trans_resize = transforms.Resize((128,128))
    img_resize = trans_resize.forward(img_tensor)
    ```

## torchvision中数据集的使用

1. `torchvision.datasets.CIFAR10()`

![image-20220705013155205](F:\Typora\pytorch\1.pytorch基础.assets\image-20220705013155205.png)

![image-20220705014745508](F:\Typora\pytorch\1.pytorch基础.assets\image-20220705014745508.png)

2. `cifar-10数据集`

   ```tex
   The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.
   ```

   

## torch中的dim

```python
# 以torch.min函数为例，dim=0时，表示每一列，dim=1时，表示每一行
import torch
a = torch.Tensor([
    [1,2,3],
    [4,5,6]
])	# a.shape = Tensor.size(2,3)
print(torch.min(a, 0))
'''
torch.return_types.min(
values=tensor([1., 2., 3.]),
indices=tensor([0, 0, 0]))
'''
print(torch.min(a, 1))
'''
torch.return_types.min(
values=tensor([1., 4.]),
indices=tensor([0, 0]))
'''
```

## with torch.no_grad()

```tex
1. with 语句适用于对资源进行访问的场合，确保不管使用过程中是否发生异常都会执行必要的“清理”操作，释放资源，比如文件使用后自动关闭／线程中锁的自动获取和释放等。
```

- `requires_grad`

  ```tex
  1. 在pytorch中，tensor有一个requires_grad参数，如果设置为True，则反向传播时，该tensor就会自动求导。tensor的requires_grad的属性默认为False,若一个节点（叶子变量：自己创建的tensor）requires_grad被设置为True，那么所有依赖它的节点requires_grad都为True（即使其他相依赖的tensor的requires_grad = False）。
  2. 当requires_grad设置为False时,反向传播时就不会自动求导了，因此大大节约了显存或者说内存。
  ```

- `with torch.no_grad的作用`

  ```tex
  % 一般用在测试和验证的过程
  1. 在该模块下，所有计算得出的tensor的requires_grad都自动设置为False。
  2. 即使一个tensor（命名为x）的requires_grad = True，在with torch.no_grad计算，由x得到的新tensor（命名为w-标量）requires_grad也为False，且grad_fn也为None,即不会对w求导。
  ```

  

## scatter()、scatter_()

```tex
1. scatter() 和 scatter_() 的作用一样。
2. 不同之处在于 scatter() 不会直接修改原来的 Tensor，而 scatter_() 会在原来的基础上对Tensor进行修改。
```

- `用法`

  ```tex
  1. scatter(dim, index, src)将src中数据根据index中的索引按照dim的方向进行填充。
  % dim=0 的填充规则
  self[ index[i][j] ] [j] = src[i][j]
  
  % dim=1 的填充规则
  self [i] [ index[i][j] ] = src[i][j]
  
  2. index必须是torch.LongTensor类型
  	index行数必须和src行数相同，且下标不能超过output的列数。
  	index列数不受限制。
  	src列数必须要小于等于进行填充的tensor列数（或者只是一个标量也可以）。
  % scatter() 用于 one-hot 编码
  ```

  > ![image-20220921165216547](F:\Typora\pytorch\1.pytorch基础.assets\image-20220921165216547.png)
  >
  > ![image-20220921165254534](F:\Typora\pytorch\1.pytorch基础.assets\image-20220921165254534.png)

- `scatter用于one-hot编码`

  ```python
  class_num = 10
  batch_size = 4
  label = torch.LongTensor(batch_size, 1).random_() % class_num
  #tensor([[6],
  #        [0],
  #        [3],
  #        [2]])
  torch.zeros(batch_size, class_num).scatter_(1, label, 1)
  #tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
  #        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
  #        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
  #        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])
  ```
