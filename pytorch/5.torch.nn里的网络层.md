[TOC]

# 卷积层

## nn.Conv2d()二维卷积层

```tex
卷积的动画：https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md#transposed-convolution-animations
```

​				[卷积动画](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md#transposed-convolution-animations)

1. 参数

   ```python
   torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
   ```

   > ![image-20220713160124684](./pic\image-20220713160124684.png)
   >
   > - N是batch_size
   > - C是通道数
   > - ★是卷积运算

   ```
   kernel_size, stride, padding, dilation可以写成：
   1. 单个int数字，表示h == w
   2. tuple元组，(h, w)
   ```
   
   > 计算 output 形状
   >
   > ![image-20220713161457520](./pic/\image-20220713161457520.png)



# 池化层

## nn.MaxPool2d()二维的最大值池化

```tex

```

1. `参数`

   ```python
   torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)
   ```

   > ![image-20220714225356058](F:\Typora\pytorch\5.torch.nn里的网络层.assets\image-20220714225356058.png)
   >
   > - padding 值为0时，默认填充负无穷 
   > - ceil_mode 取整

# 非线性激活函数

```tex
负责将神经元的输入映射到输出端。如下图，在神经元中，输入的 inputs 通过加权，求和后，还被作用了一个函数，这个函数就是激活函数。
```

> ![img](https://pic3.zhimg.com/80/v2-c1f3ccd9bb27cd3f2af244c1d2341732_720w.jpg)



## nn.Sigmoid()

```tex
优点：
	Sigmoid函数的优点在于它可导，并且值域在0到1之间，使得神经元的输出标准化。
缺点：
    （1）梯度消失：Sigmoid 函数值在趋近 0 和 1 的时候函数值会变得平坦，梯度趋近于 0。
    （2）不以零为中心：sigmoid函数的输出恒为正值，不是以零为中心的，这会导致权值更新时只能朝一个方向更新，从而影响收敛速度。
    （3）计算成本高昂：exp() 函数与其他非线性激活函数相比，计算成本高昂。
    （4）梯度爆炸：x值在趋近0的左右两边时，会造成梯度爆炸情况。
```

1. `公式和图像`

   > ![image-20220715010617929](F:\Typora\pytorch\5.torch.nn里的网络层.assets\image-20220715010617929.png)
   >
   > ![../_images/Sigmoid.png](https://pytorch.org/docs/1.10/_images/Sigmoid.png)



## nn.LogSigmoid()

```
函数值域：[0, 负无穷]， 一定程度上解决了梯度消失的问题
```

1. `公式和图像`

   > ![image-20220715005352334](F:\Typora\pytorch\5.torch.nn里的网络层.assets\image-20220715005352334.png)
   >
   > ![../_images/LogSigmoid.png](https://pytorch.org/docs/1.10/_images/LogSigmoid.png)



## nn.Tanh()双曲正切激活函数

```
优点：输出值以 0 为中心，解决了sigmoid函数输出值只为正，梯度只向一个方向更新的问题。
缺点：依然存在sigmoid中梯度消失和爆炸的问题和指数运算计算量大的问题。
```

1. `公式和图像`

   > ![image-20220715010812167](F:\Typora\pytorch\5.torch.nn里的网络层.assets\image-20220715010812167.png)
   >
   > ![../_images/Tanh.png](https://pytorch.org/docs/1.10/_images/Tanh.png)



## nn.ReLU()线性整流函数

```python
# inplace: 直接复制网络中传下来的Tensor
torch.nn.ReLU(inplace=False)
```

```tex
修正线性单元。
优点：
（1）收敛速度快，并且在正值区域（x> 0 ）可以对抗梯度消失问题。
（2）相比于 sigmoid，由于稀疏性，时间和空间复杂度更低；不涉及成本更高的指数运算。
缺点：
（1）不以零为中心：和 Sigmoid 激活函数类似，ReLU 函数的输出不以零为中心，因此只存在正向梯度。
（2）负值区域（x< 0 ）存在梯度消失问题。如果 x < 0，则神经元保持非激活状态，且在反向传播过程中「杀死」梯度。这样权重无法得到更新，网络无法学习。
```

1. `公式图像`

   > ![image-20220715011142138](F:\Typora\pytorch\5.torch.nn里的网络层.assets\image-20220715011142138.png)
   >
   > ![../_images/ReLU.png](https://pytorch.org/docs/1.10/_images/ReLU.png)
   >
   > 求导后图像为：
   >
   > ![image-20220715011326412](F:\Typora\pytorch\5.torch.nn里的网络层.assets\image-20220715011326412.png)



## nn.LeakyReLU()渗漏型整流线性单元

```python
torch.nn.LeakyReLU(negative_slope=0.01, inplace=False)
```

```tex
优点：
（1）解决了ReLu激活函数中负值区域（x < 0 ）的梯度消失问题。
缺点：
（1）神经网络不学习 negative_slope 值
```

1. `公式图像`

   > ![image-20220715011623734](F:\Typora\pytorch\5.torch.nn里的网络层.assets\image-20220715011623734.png)
   >
   > ![image-20220715011741481](F:\Typora\pytorch\5.torch.nn里的网络层.assets\image-20220715011741481.png)



## nn.ELU()指数线性单元

```python
torch.nn.ELU(alpha=1.0, inplace=False)
```

```tex
优点：
（1）能避免ReLu负值区域（x<0）梯度消失问题
（2）能得到负值输出，这能帮助网络向正确的方向推动权重和偏置变化
缺点：
（1）由于包含指数运算，所以计算时间更长
（2）神经网络不学习 α 值
```

1. `公式图像`

   > ![image-20220715012034625](F:\Typora\pytorch\5.torch.nn里的网络层.assets\image-20220715012034625.png)
   >
   > ![image-20220715012047822](F:\Typora\pytorch\5.torch.nn里的网络层.assets\image-20220715012047822.png)

## nn.Softmax()

```python
torch.nn.Softmax(dim=None)
# return_type: a Tensor of the same dimension and shape as the input with values in the range [0, 1]
```

```tex
Softmax函数比较适合作为多分类模型的激活函数，一般会与交叉熵损失函数相配。
Softmax函数的输出结果是0到1之间的概率值，对应着输入数据属于某个类别的概率，因此适合于多分类模型。通常，Softmax函数只应用于输出层。
```

1. `公式图像`

   > ![image-20220715012653077](F:\Typora\pytorch\5.torch.nn里的网络层.assets\image-20220715012653077.png)
   >
   > ![image-20220715012719979](F:\Typora\pytorch\5.torch.nn里的网络层.assets\image-20220715012719979.png)

2. `代码测试`

   

   ```python
   # torch.randn( (c,h,w,...) ) 返回一个输入形状的 tensor
   m = nn.Softmax(dim=1)
   input = torch.randn(2, 3)
   output = m(input)
   ```

   - 输出结果 

     > ![image-20220715013745804](F:\Typora\pytorch\5.torch.nn里的网络层.assets\image-20220715013745804.png)

## 激活函数的选择

1. `根据经验`
   - Sigmoid函数比较适合于二分类模型。
   - 使用Sigmoid函数和tanh函数，要注意梯度消失问题。
   - ReLU函数是应用比较广泛的激活函数，可以作为你的默认选项。
   - 如果网络中存在大量未激活神经元，可以考虑leaky ReLU函数。
   - ReLU函数应该只用于隐藏层。
   - 如果是回归模型，在输出层上可以使用线性激活函数。

# 归一化层

1. `为什么要提出归一化层`

   ```tex
   	Batch Normalization的原论文作者给了Internal Covariate Shift一个较规范的定义：在深层网络训练的过程中，由于网络中参数变化而引起内部结点数据分布发生变化的这一过程被称作Internal Covariate Shift。
   	带来的问题：
   	（1）上层网络需要不停调整来适应输入数据分布的变化，导致网络学习速度的降低
   	（2）网络的训练过程容易陷入梯度饱和区，减缓网络收敛速度
   ```

2. `使用BN算法解决ICS问题`

   ```tex
   （1）BN使得网络中每层输入数据的分布相对稳定，加速模型学习速度
   （2）BN使得模型对网络中的参数不那么敏感，简化调参过程，使得网络学习更加稳定
   （3）BN允许网络使用饱和性激活函数（例如sigmoid，tanh等），缓解梯度消失问题
   （4）BN具有一定的正则化效果
   ```

3. `BN层的主要作用`

   ```tex
   1. 加快网络的训练和收敛的速度
   2. 控制梯度爆炸防止梯度消失
   3. 防止过拟合
   ```

## nn.BatchNorm2d

```python
# 目的是使一批(batch)feature map满足均值为0，方差为1的分布规律
# 用来加速训练
'''
	1. batch_sieze尽可能设置的大点，设置的越大求得均值和方差越接近整个训练集得均值和方差
	2. 一般放在卷积层(Conv)和激活层(如Relu)之间，且卷积层不要使用bias
'''
torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)
```

1. `公式`

   > ![image-20220715161422176](F:\Typora\pytorch\5.torch.nn里的网络层.assets\image-20220715161422176.png)
   >
   > ![image-20220715161428453](F:\Typora\pytorch\5.torch.nn里的网络层.assets\image-20220715161428453.png)

2. `参数`

   ```tex
   num_features: 输入的通道数
   eps: 分母上的偏置
   affine: 使能γ和β
   ```

   > ![image-20220715162044023](F:\Typora\pytorch\5.torch.nn里的网络层.assets\image-20220715162044023.png)

# 循环层

```tex
多用于NLP，网络的输入上下文有相关性的场景。
```

[RNN动画](https://www.bilibili.com/video/BV1z5411f7Bm)



# Transformer层



## nn.GRU

```python
torch.nn.GRU(*args, **kwargs)
```

代码实现和原理见dl_limu

- 常用参数
  - `input_size`：X的维度
  - `hidden_size`：隐层维度
  - `num_layers`：叠几层GRU
- 输入：$(L, H_{in}), (L, N, H_{in}), (N, L, H_{in}),N是batch大小$
- 输出：$(D * num\_layers, H_{out}), (D * num\_layers, N, H_{out})$ ，其中 $D$ 是是否为双向网络，$H$ 一般是 `hidden_size`。





# 线性层

## nn.Linear

```python
torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)
```

1. `公式`

> ![image-20220715164238931](F:\Typora\pytorch\5.torch.nn里的网络层.assets\image-20220715164238931.png)

2. `参数`

   ```tex
   in_features: size of each input sample
   out_features: size of each output sample
   ```



# 嵌入层

## nn.Embedding

```python
torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None, device=None, dtype=None)
```

使用最多的是前两个参数：`num_embeddings` 和 `embedding_dim` 。

- `num_embeddings` ：嵌入层字典的大小，即一共有多少个单词

- `embedding_dim`：每个嵌入向量的大小

- 输入 $(*)$ ：IntTensor or LongTensor of arbitrary shape containing the indices to extract

- 输出 $(*, H)$ ：$*$ 是输入的形状，$H$ =  `embedding_dim` 

- 例子

  ```python
  # an Embedding module containing 10 tensors of size 3
  embedding = nn.Embedding(10, 3)	# 一共有整个嵌入空间中只有10个单词可以映射，也就是说，输入的索引只能是0-9
  # a batch of 2 samples of 4 indices each
  input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])	# (2, 4) 每个索引对应嵌入空间中的一个单词，这里每个batch有4个单词
  embedding(input)
  # 输出(2, 4, 3)
  tensor([[[-0.0251, -1.6902,  0.7172],
           [-0.6431,  0.0748,  0.6969],
           [ 1.4970,  1.3448, -0.9685],
           [-0.3677, -2.7265, -0.1685]],
  
          [[ 1.4970,  1.3448, -0.9685],
           [ 0.4362, -0.4004,  0.9400],
           [-0.6431,  0.0748,  0.6969],
           [ 0.9124, -2.3616,  1.1151]]])
  
  ```

  

# 损失函数

1. `什么是损失函数`

   ```tex
   损失函数（loss function）就是用来度量模型的预测值f(x)与真实值Y的差异程度的运算函数，它是一个非负实值函数，通常使用L(Y, f(x))来表示，损失函数越小，模型的鲁棒性就越好。
   ```

2. `为什么使用损失函数`

   ```tex
   损失函数使用主要是在模型的训练阶段，每个批次的训练数据送入模型后，通过前向传播输出预测值，然后损失函数会计算出预测值和真实值之间的差异值，也就是损失值。得到损失值之后，模型通过反向传播去更新各个参数，来降低真实值与预测值之间的损失，使得模型生成的预测值往真实值方向靠拢，从而达到学习的目的。
   ```



## nn.L1Loss

```python
# input和target差的绝对值
torch.nn.L1Loss(size_average=None, reduce=None, reduction='mean')
```

1. `计算公式`

   > ![image-20220715170332301](F:\Typora\pytorch\5.torch.nn里的网络层.assets\image-20220715170332301.png)

2. `调用`

   ```python
   loss = nn.L1Loss()
   input = torch.randn(3, 5, requires_grad=True)
   target = torch.randn(3, 5)
   output = loss(input, target)
   output.backward()
   ```

   

## nn.MSELoss

```python
# 方差
torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')
```

1. `计算公式`

   > ![image-20220715170558791](F:\Typora\pytorch\5.torch.nn里的网络层.assets\image-20220715170558791.png)



## nn.CrossEntropyLoss

```python
# 交叉熵
torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=- 100, reduce=None, reduction='mean', label_smoothing=0.0)
```

1. `计算公式`

   - `二分类`

     > 这里的 y 是 one-hot 向量
     >
     > ![image-20220716004554194](F:\Typora\pytorch\5.torch.nn里的网络层.assets\image-20220716004554194.png)
   
   - `多分类`
   
     > ![image-20220716004649533](F:\Typora\pytorch\5.torch.nn里的网络层.assets\image-20220716004649533.png)



