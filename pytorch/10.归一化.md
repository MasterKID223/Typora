### [torch.nn.functional.normalize](https://pytorch.org/docs/stable/generated/torch.nn.functional.normalize.html)

- 函数

  ```
  torch.nn.functional.normalize(input, p=2.0, dim=1, eps=1e-12, out=None)
  ```

- 用法

  对输入做 $L_p$ 归一化公式：

  对输入的向量 $(n_0, ..., n_{dim}, ..., n_k)$ ，每个 $n_{dim}$ 元素的向量 $v$ 都沿 `dim` 的方向：
  $$
  v = \frac{v}{max(||v||_p, \epsilon)}
  $$

### BN和LN的区别

BN和LN都是用于深度神经网络中的归一化技术，旨在加速模型的收敛并提高模型性能。它们分别代表批归一化（Batch Normalization）和层归一化（Layer Normalization），下面将分别介绍它们的定义和区别：

1. 批归一化（Batch Normalization，简称BN）：
   - 定义：BN是一种在深度神经网络中应用的归一化技术，通过对每个批次的输入进行归一化来调整神经网络的激活值分布。
   - 操作：在每个神经网络层的输出上，计算批次的均值和方差，然后将输出标准化为零均值和单位方差。最后，通过缩放和平移参数来恢复标准化的值，并将其作为下一层的输入。
   - 优点：BN可以加速模型的收敛速度，减少对初始权重的敏感性，使得网络更容易训练。此外，BN还有一定的正则化效果，可以减少模型的过拟合。
   - 缺点：BN在训练过程中引入了额外的计算开销，并且对于较小的批次大小可能会产生较大的估计误差。
2. 层归一化（Layer Normalization，简称LN）：
   - 定义：LN是一种类似于BN的归一化技术，但它是对每个神经网络层的输出进行归一化，而不是对批次的输入进行归一化。
   - 操作：在每个神经网络层的输出上，计算每个样本的均值和方差，然后将输出标准化为零均值和单位方差。最后，通过缩放和平移参数来恢复标准化的值，并将其作为下一层的输入。
   - 优点：LN具有与BN类似的加速收敛和正则化效果，并且对于较小的批次大小具有较低的估计误差。
   - 缺点：LN相对于BN在训练过程中的计算开销较小，但在推理过程中可能会比BN更加耗时。

区别：

- BN是对每个批次的输入进行归一化，而LN是对每个神经网络层的输出进行归一化。
- BN的计算是基于每个批次的均值和方差，而LN的计算是基于每个样本的均值和方差。
- BN对于较小的批次大小可能会产生较大的估计误差，而LN对于较小的批次大小具有较低的估计误差。
- BN在训练过程中引入了额外的计算开销，而LN的计算开销相对较小。
- 在推理过程中，LN相对于BN可能会更加耗时。

需要注意的是，除了BN和LN，还有其他类型的归一化技术，如实例归一化（Instance Normalization）和组归一化（Group Normalization），它们在不同的应用场景中可能会有不同的效果。选择适当的归一化技术通常取决于具体的问题和网络结构。

