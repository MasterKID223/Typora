### SGD实现随机梯度下降算法

```python
torch.optim.SGD(params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False)
```

1. 参数
   - **params** (*iterable*) – 待优化的`dict`参数

   

2. 优缺点

    ```tex
    优点：（1）使用mini-batch的时候，可以收敛得很快
    缺点：（1）在随机选择梯度的同时会引入噪声，使得权值更新的方向不一定正确
    	（2）不能解决局部最优解的问题
    ```

3. 使用

    ```python
    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)
    optimizer.zero_grad()
    loss_fn(model(input), target).backward()
    optimizer.step()
    ```



### ASGD随机平均梯度下降


```python
# 是用空间换时间的一种SGD
torch.optim.ASGD(params, lr=0.01, lambd=0.0001, alpha=0.75, t0=1000000.0, weight_decay=0)
```

1. 参数
   - **ambd** ([*float*](https://docs.python.org/3/library/functions.html#float)*,* *optional*) – 衰减项 (default: 1e-4)
   - **alpha** ([*float*](https://docs.python.org/3/library/functions.html#float)*,* *optional*) – eta更新的指数 (default: 0.75)
   - **t0** ([*float*](https://docs.python.org/3/library/functions.html#float)*,* *optional*) – 指明在哪一次开始平均化 (default: 1e6)

2. 使用同上



### AdaGrad算法

```tex
独立地适应所有模型参数的学习率，梯度越大，学习率越小；梯度越小，学习率越大
Adagrad适用于数据稀疏或者分布不平衡的数据集

优点：它可以自动调节学习率，不需要人为调节
缺点：仍依赖于人工设置一个全局学习率，随着迭代次数增多，学习率会越来越小，最终会趋近于0
```

```python
torch.optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10)
```

### AdaDelta算法

```tex
是Adagard的改进版，对学习率进行自适应约束，但是进行了计算上的简化，加速效果不错，训练速度快

优点：避免在训练后期，学习率过小；初期和中期，加速效果不错，训练速度快
缺点：还是需要自己手动指定初始学习率，初始梯度很大的话，会导致整个训练过程的学习率一直很小，在模型训练的后期，模型会反复地在局部最小值附近抖动，从而导致学习时间变长
```

```python
torch.optim.Adadelta(params, lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)
```

### RMSProp均方根传递

```tex
RProp的改进版，也是Adagard的改进版
思想：梯度震动较大的项，在下降时，减小其下降速度；对于震动幅度小的项，在下降时，加速其下降速度
RMSprop采用均方根作为分母，可缓解Adagrad学习率下降较快的问题
对于RNN有很好的效果。

优点：可缓解Adagrad学习率下降较快的问题，并且引入均方根，可以减少摆动，适合处理非平稳目标，对于RNN效果很好
缺点：依然依赖于全局学习率
```

```python
# centered (bool) – 如果为True，计算中心化的RMSProp，并且用它的方差预测值对梯度进行归一化
torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)
```

### Adam

具体算法见：[[ADAM](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html)]

```tex
将Momentum算法和RMSProp算法结合起来使用的一种算法，既用动量来累积梯度，又使得收敛速度更快同时使得波动的幅度更小，并进行了偏差修正

优点：
1、对目标函数没有平稳要求，即loss function可以随着时间变化
2、参数的更新不受梯度的伸缩变换影响
3、更新步长和梯度大小无关，只和alpha、beta_1、beta_2有关系。并且由它们决定步长的理论上限
4、更新的步长能够被限制在大致的范围内（初始学习率）
5、能较好的处理噪音样本，能天然地实现步长退火过程（自动调整学习率）
6、很适合应用于大规模的数据及参数的场景、不稳定目标函数、梯度稀疏或梯度存在很大噪声的问题
```

```python
# betas (Tuple[float,float]) – 用于计算梯度以及梯度平方的运行平均值的系数（默认：0.9，0.999）
torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)
```

- 计算公式

