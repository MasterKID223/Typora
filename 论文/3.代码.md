# 代码笔记

## main_finetune.py

- `命令行参数`

  ```json
  {
      "batch_size": 64,
      "model": "vit_large_patch16",
      "input_size": (224, 224),
      "norm_pix_loss ": True,
      "mask_ratio": 0.75,
      "epochs": 800,
      "warmup_epochs": 40,
      "blr": 1.5e-4,
      "weight_decay": 0.05
  }
  ```

- `大致过程`

  ```tex
  1. 初始化分布式训练参数。
  2. 设置seed，实现可在现。
  3. 加载训练集和验证集。
  4. 初始化采样器：
  	% distributed
  	分布式取样，torch.utils.data.DistributedSampler
  	% 非分布式
  	sampler_train: 随机取样
  	sampler_val: 顺序取样
  5. 初始化SummaryWriter
  	% 当前进程优先级最高，log_dir非空，不是val过程
  	log_writer = SummaryWriter(log_dir=args.log_dir)
  	% 否则
  	log_writer = None
  6. 初始化 data_loader_train | val。
  7. 初始化mixup函数，用于数据增广，可以增加模型的泛化能力。(mixup: Beyond empirical risk minimization[J])
  8. 模型初始化。
  % arg.finetune and not args.eval
  	9. 加载finetune参数。
  	checkpoint = torch.load(args.finetune, map_location='cpu')
  	在checkpoint_model里删除一些参数
  	插入位置embedding, interpolate_pos_embed(model, checkpoint_model)
  	加载预训练模型
  	（？） 
  	手动初始化fc层
  10. 初始化学习率。
  % 分布式训练初始化
  	pass
  11. 使用逐层lr衰退，构建优化器。optimizer = torch.optim.AdamW(param_groups, lr=args.lr)
  12. (?)初始化loss_scaler。loss_scaler = NativeScaler()
  13. 初始化损失函数。criterion = torch.nn.CrossEntropyLoss()
  14. 加载模型。misc.load_model(args=args, model_without_ddp=model_without_ddp, optimizer=optimizer, loss_scaler=loss_scaler)
  % 验证
  	pass
  15. 开始训练，验证，log。
  ```

## engine_finetune.py

- `train_one_epoch`

  ```tex
  % train_one_epoch
  1. 初始化metric_logger记录训练过程中用的参数。{ header, "?", eta, {meters}, iter_time, data_time },
  其中{meters} = MetricLogger.meters
  2. (?) for enumerate
  	学习率调度，
  	mixup函数，
  	计算损失，
  	更新梯度，
  	打印日志。
  ```
  
  



## main_pretrain.py和main_finetune不一样的地方

```tex
% 在for epoch之后，在train_one_epoch中
1. pretrain.py中没有(criterion,args.clip_grad, mixup_fn,)，没有test过程。
2. 在pretrain的train_one_epoch中只取了(sample, )
3. fine_tuning中没有数组增强。
```

```tex
% 在预训练过程中，仅计算masked像素的loss
（？）没看到对应代码
```



## main_linprobe.py

```tex
% linprobe多的操作
1. 初始化数据集之前，对数据的增强是weak augmentation，而pretrain中是simple augmentation。
2. 用BN修改了模型的头部，优化器也只更新模型头部的参数（梯度）。
```



## 三个过程的损失函数和优化器

- `pretrain, finetune, linpobe -> loss_fn`

> ![image-20220917222859048](F:\Typora\论文\3.代码.assets\image-20220917222859048.png)	
>
> ![image-20220917223000216](F:\Typora\论文\3.代码.assets\image-20220917223000216.png)

- `optimizer`

  > ![image-20220917223046729](F:\Typora\论文\3.代码.assets\image-20220917223046729.png)
  >
  > ![image-20220917223107113](F:\Typora\论文\3.代码.assets\image-20220917223107113.png)
  >
  > ![image-20220917223024785](F:\Typora\论文\3.代码.assets\image-20220917223024785.png)
