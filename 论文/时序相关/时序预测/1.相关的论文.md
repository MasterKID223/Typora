- 综述：

  [原理+论文+实战：60篇由浅入深的时间序列预测/分类教程汇总](https://blog.csdn.net/weixin_39653948/article/details/105571760#t1)

  [Transformer应用于时序任务的综述【2022by阿里达摩院】](https://arxiv.org/pdf/2202.07125.pdf)

  [12篇顶会论文，深度学习时间序列预测经典方案汇总](https://zhuanlan.zhihu.com/p/495427635)



来源：Autoformer论文的参考文文献（高瀚师兄10.25集体学习讲的论文）。

时序预测：

- 深度预测模型：

  [25] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term temporal patterns with deep neural networks. In SIGIR, 2018.
  
  [29] Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-BEATS: Neural basis
  expansion analysis for interpretable time series forecasting. ICLR, 2019.
  
  [34] David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. DeepAR: Probabilistic forecasting
  with autoregressive recurrent networks. Int. J. Forecast., 2020.
  
  [35] Rajat Sen, Hsiang-Fu Yu, and Inderjit S. Dhillon. Think globally, act locally: A deep neural network
  approach to high-dimensional time series forecasting. In NeurIPS, 2019.
  
  [41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
  Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.
  
- 基于Transformer的预测模型（自注意力 -> 稀疏注意力）

  [48] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.
  Informer: Beyond efficient transformer for long sequence time-series forecasting. In AAAI, 2021.	

  [23] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In ICLR, 2020.

  [26] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan.
  Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. In NeurIPS, 2019.

