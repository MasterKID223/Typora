## A Time Series Is Worth 64 Words_Long-Term Forcasting With Transformers

作者：Yuqi Nie（普林斯顿大学）

来源：ICLR2023

论文：[[arxiv](https://arxiv.org/pdf/2211.14730)]

代码：[[github](https://github.com/yuqinie98/PatchTST)]

引用数：1

参考：[]

关键词：PatchTST

### 创新点

多元时序数据预测的自监督表征学习基于Transformer的模型。有两部分组成：

- 在送入Transformer之前，把时序数据分割成子序列级别的patch。
- 每个通道独立，每个通道都包含一个单元时间序列，所有通道的时序共享embeding权重。