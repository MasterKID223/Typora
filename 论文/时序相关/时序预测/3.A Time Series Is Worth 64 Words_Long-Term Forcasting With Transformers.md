## A Time Series Is Worth 64 Words_Long-Term Forcasting With Transformers

作者：Yuqi Nie（普林斯顿大学）

来源：ICLR2023

论文：[[arxiv](https://arxiv.org/pdf/2211.14730)]

代码：[[github](https://github.com/yuqinie98/PatchTST)]

引用数：1

参考：[]

关键词：PatchTST（patch time series Transformer），通道独立结构。

### 创新点

多元时序数据预测的自监督表征学习基于Transformer的模型。有两部分组成：

- 在送入Transformer之前，把时序数据分割成子序列级别的patch。
- 每个通道独立，每个通道都包含一个单元时间序列，所有通道的时序共享embeding权重。

![image-20230215151259208](pic/image-20230215151259208.png)

贡献：（表1）

1. 普通的Transformer是$O(N^2)$的。N是tokens的数量。对时序数据，如果不进行预处理，N就等于序列长度$L$。划分patch：$N \approx L/S$。这样计算量就减少了。

2. 从更大的滑动窗口中学习。从表1中看出，窗口大小L从96到336，MSE变小。然而窗口变大导致内存占用高。因为时间序列经常携带冗余的时间信息。之前的一些工作使用下采样或者稀疏注意力减少计算，也取得了比较好的预测结果。当$L=380$时，作者这样划分patch，在380的窗口内，每4步取一个token（最后一点补够4个），那么输入的token长度是$N=96$。通过token的方式，把相邻的时间点组到一起，比把单个时间点当成token输入进去效果更好。
3. 表征学习的能力更好。Transformer结构适合时序预测任务。

### 结论



### 实验结果

监督方式的结果：

<img src="pic/image-20230215154152025.png" alt="image-20230215154152025" style="zoom:25%;" />

自监督方式的结果：

<img src="pic/image-20230215154216628.png" alt="image-20230215154216628" style="zoom:25%;" />

### 3. 方法

**问题定义** 回看窗口大小是<img src="pic/image-20230216094203403.png" alt="image-20230216094203403" style="zoom:67%;" />，$x_t$是t时刻的$M$维向量，任务是预测未来$T$步的数据<img src="pic/image-20230216095309456.png" alt="image-20230216095309456" style="zoom:67%;" />。

<img src="pic/image-20230216095353354.png" alt="image-20230216095353354" style="zoom:25%;" />

<img src="pic/image-20230216095417064.png" alt="image-20230216095417064" style="zoom: 80%;" />

#### 3.1. 模型结构

- **前向过程** 

  $L$的窗口内，第$i$维的序列这样表示：<img src="pic/image-20230216095859935.png" alt="image-20230216095859935" style="zoom:67%;" />，整个时序数据分成了$M$个独立的序列<img src="pic/image-20230216095951715.png" alt="image-20230216095951715" style="zoom:67%;" />，每个序列独立的输入到Transformer中。由Transformer的backbone做出预测<img src="pic/image-20230216100152342.png" alt="image-20230216100152342" style="zoom:67%;" />。

- **Patching** 

  把分好的单个序列分成Patch（以重叠或不重叠的方式）。patch长度是$P$，stride是两个连续patch之间的非重叠区域的长度，那么一个序列划分成：<img src="pic/image-20230216100530844.png" alt="image-20230216100530844" style="zoom:67%;" />，$N$是patch的数量，<img src="pic/image-20230216100602065.png" alt="image-20230216100602065" style="zoom:67%;" />。做patch的过程中，要把最后一个值<img src="pic/image-20230216100708065.png" alt="image-20230216100708065" style="zoom:67%;" />重复$S$次，把序列补成$P$的整数倍。最后token的数量减少为（不重叠方式）：$L \rightarrow L/S$。

- **Transformer Encoder** （产生预测结果）

  首先把每个patch映射成$D$维向量<img src="pic/image-20230216101042450.png" alt="image-20230216101042450" style="zoom:67%;" />（线性层），给$N$个patch加上位置编码<img src="pic/image-20230216101129565.png" alt="image-20230216101129565" style="zoom:67%;" />（保存时间顺序信息），最后每个patch：<img src="pic/image-20230216101232026.png" alt="image-20230216101232026" style="zoom:67%;" />。然后过多头注意力层。Encoder多头之后的Norm是BN。从Encoder出来的维度：<img src="pic/image-20230216101539537.png" alt="image-20230216101539537" style="zoom:67%;" />。最后过一个全连接层产生预测结果：<img src="pic/image-20230216101656604.png" alt="image-20230216101656604" style="zoom:67%;" /><img src="pic/image-20230216101706538.png" alt="image-20230216101706538" style="zoom:67%;" />。

- **损失函数**

  MSE损失计算预测值和真实值之间的差异。每个通道的损失：

  ![image-20230216101836335](pic/image-20230216101836335.png)

  $M$个通道的平均损失：
  ![image-20230216101913491](pic/image-20230216101913491.png)

- **Instance Normalization** （patch之前的操作）

  （？）用来缩小训练数据和测试数据的偏移。就是把每个序列<img src="pic/image-20230216102142786.png" alt="image-20230216102142786" style="zoom:67%;" />用mean=0，std=1做归一化。

  在patching之前对每个序列做归一化，得到最后的结果后，在反归一化（保留mean，std，做归一化的你操作）。

  > In essence, we normalize each $x^{(i)}$ before patching and the mean and deviation are added back to the output prediction.

  

#### 3.2. 表征学习

用时许数据的特征进行训练。提取特征用自监督与训练模型（如NLP的Bert和CV的MAE）。掩掉一部分数据，然后重建出来。

MAE已经用到时序分类和识别任务上，效果很好：[A transformer-based framework for multivariate time series representation learning](https://dl.acm.org/doi/pdf/10.1145/3447548.3467401)（表示在时序Transforme上，BatchNorm的效果好于LayerNorm）。这篇作者只是把单个时间步当成token输入进去。这样做mask就有两个问题：

第一，只在单个时间步这个粒度上做mask，mask 一个时间点的话，直接根据相邻点插值就可以重建，这就完全没必要学习了，而 mask一个patch来重建的话则更有意义更有难度。上面的作者就提出了一个复杂的随机策略解决这个的问题：随机mask，每个mask掉的大小不同。

第二，最后输出预测结果的输出层的设计比较麻烦。把$L$个<img src="pic/image-20230216104419223.png" alt="image-20230216104419223" style="zoom:67%;" />的向量，映射成长为$T$的$M$维向量，这一层的参数量就是：<img src="pic/image-20230216104556004.png" alt="image-20230216104556004" style="zoom:67%;" />。这四个数字有一个很大就会导致，整个矩阵过大，当训练的数据不够时，就会导致严重的过拟合。回到本篇文章，作者提出的PatchTST克服了这个问题（<font color=red>？</font>）：把预测头去掉，连接一个<img src="pic/image-20230216112127612.png" alt="image-20230216112127612" style="zoom:67%;" />的线性层。

这就很直观了，直接 mask 掉一些 patch 然后进行重建即可。对于每一个 token（patch），它通过 Transformer Encoder 后输出维度是D，由于该 patch 本身的长度是P，因此要重建它的话，再加上一个D✖️P的 Linear 层即可。

### A 附录

#### A.1. 实验细节

##### A.1.1. 数据集

![image-20230216112403987](pic/image-20230216112403987.png)

Exchange-rate是金融数据集，和其他领域的数据集的属性不一样。

Are transformers effective for time series forecasting?表明，通过简单地重复回看窗口中的最后一个值，MSE在汇率数据集上的损失可以超过或与最佳结果相当。因此，作者谨慎地将其纳入基准。
