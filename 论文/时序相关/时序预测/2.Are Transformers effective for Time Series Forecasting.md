## Are Transformers Effective for Time Series Forecasting?

作者：Ailing Zeng（香港中文大学）

来源：AAAI 2023

论文：[[arxiv](https://arxiv.org/pdf/2205.13504)]

代码：[[github](https://github.com/cure-lab/LTSF-Linear)]

引用数：17

参考：[[知乎1](https://zhuanlan.zhihu.com/p/569194246)] [[知乎2](https://zhuanlan.zhihu.com/p/544622984)]

关键词：Linear, NLinear, and DLinear

### 摘要

最近在LTSF上有很多基于Transformer的工作。作者在这篇文章中，指出了这些方法中存在一些问题。特别地，Transformer是在长序列中提取元素之间相关行的最成功的方法。然而，在时间序列建模中，我们需要从一组有序的连续点中提取时间关系。在Transformer中用位置编码和tokens可以方便的保留顺序信息，但置换不变（permutation-invariant）自注意机制的性质不可避免地导致时间信息丢失。

为了验证我们的想法，我们使用了一个非常简单的一层线性模型（LTSF-Linear）和Transformer的方法比较。结果显示，一层线性模型在大部分情况下，效果更好。令人惊讶的是，我们的结果显示，LTSF Linear在所有情况下都优于现有的复杂Transformerbased模型，并且通常有很大的差距（20%∼ 50%)。

### 介绍和相关工作

参考第一篇博客。



### 2. TSF问题定义

对于包含$C$个变量的时间序列，给定历史序列<img src="./pic/image-20230123185436811.png" alt="image-20230123185436811" style="zoom:67%;" />，$L$是滑动窗口大小，$X_i^t$是时间步$t$的第$i$个值。

时序预测任务是预测<img src="./pic/image-20230123185639893.png" alt="image-20230123185639893" style="zoom:67%;" />，在未来的$T$个时间步。

IMS预测[23]学习一个单步的预测器，并迭代的获得多步于此。或者，DMS直接一次预测多步。

**IMS和DMS比较：** 与DMS预测结果相比，由于采用了自回归模式，IMS预测的方差较小，但不可避免地会受到误差累积效应的影响。因此，**当有一个高度准确的单步的predictor，且T相对较小时，IMS更可取。相比之下，当难以获得无偏的单步预测模型或T较大时，DMS预测会生成更准确的预测（make sense）**。



### 3. 基于Transformer的LSTF解决方案

基于Transformer的模型可以并行处理（多亏了多头注意力机制）。

当在LSTF任务上应用Transformer时，时空复杂度高，还有自回归解码器设计造成的误差累积。Informer解决了这些问题，并提出了一种降低复杂性的新型Transformer架构和DMS预测策略。后来，更多Transformer变体将各种时间序列特性引入其模型，以提高性能或效率[18,28,31]。我们将现有基于Transformer的LTSF解决方案的设计元素总结如下（见图1）。

<img src="./pic/image-20230123213803309.png" alt="image-20230123213803309" style="zoom: 33%;" />

**Time series decomposition:** 在数据预处理中，在TSF任务中，0均值归一化比较常见。Autoformer首次在每个神经块后应用了季节趋势分解，这是一个时序分析中标准的方法，使原始数据更可信[6,13]。特别的，他们使用了一个移动平均核在输入的序列上，去提取时序中的趋势周期性（trend-cyclical）因素。原始序列核趋势分量之间的差异被当作是**季节分量**。在Autoformer分解方案的基础上，FEDformer[31]进一步提出了专家策略的混合，以将通过移动平均核提取的趋势分量与各种核大小混合。

**Input embedding strategies:** Transformer中的自注意力层不能保留时序的位置信息。然而，局部的位置信息（时间序列的顺序）是重要的。并且，全局的时间信息，比如层次的时间戳（周，月，年）和未知（agnostic）时间戳（假期和事件）也是有信息性的[30]。为了加强输入时间序列的时间上下文，一些基于Transformer SOTA模型中的一些embed设计被应用了，像固定位置编码，通道映射嵌入，可学习的时间嵌入。带有时间卷积层的时间嵌入[16]和可学习的时间戳[28]在这两篇文章中被介绍。

**Self-attention schemes:** LogTrans和Pyraformer给自注意力机制引入了稀疏偏置。Informer和FEDformer在自注意力矩阵中使用了低秩属性。

| 方法                | 复杂度        | 备注                                       |
| ------------------- | ------------- | ------------------------------------------ |
| vanilla Transformer | $O(L^2)$      | 原始的Transformer                          |
| LogTrans            | $O(L \log L)$ | 使用了log稀疏的mask                        |
| Pyraformer          | $O(L)$        | 金字塔注意力，层次地捕获多尺度时间依赖     |
| Informer            | $O(L \log L)$ | 概率稀疏自注意力机制和一个自注意力蒸馏操作 |
| FEDformer           | $O(L)$        | 傅里叶加强块和随机选择小波增强块           |

 **Decoders:** 原始的Transformer解码器的输出是一种自回归的形式，又慢又累积误差，尤其对于长时期的预测来说。Informer为DMS设计了一个生成式的解码器。其他Transformer变种应用了相同的DMS策略。例如，Pyraformer使用了一个全连接层来连接时空轴，作为解码器。Autoformer从趋势周期成分和季节成分的叠加自相关机制中总结出两个细化的分解特征，以获得最终预测。FED 形成器也使用分解方案与提出的频率注意块解码的最终结果。

在时间序列的建模中，我们主要关注是一个连续点集中的时间关系，和这些元素之间的顺序，而不是对之间的关系。



### 4. 一个令人尴尬的简单的baseline

在基于Transformer的LTSF的实验中，所有对比的baseline（非Transformer）都是IMS预测技术，已知它们受到显著的误差累积效应的影响。我们假设这些工作中的性能改进很大程度上是由于其中使用的DMS策略。

<img src="../pic/image-20230214041100041.png" alt="image-20230214041100041" style="zoom: 80%;" />

<img src=".pic/image-20230123224601289.png" alt="image-20230123224601289" style="zoom: 67%;" />

为了验证这个假设，我们提出了最简单的DMS模型，即一个时间线性层，叫LTSF-Linear，作为baseline用来比较。LTSF-Linear的基本公式通过加权和运算直接回归历史时间序列，用于未来的预测(如图2所示)。数学上的表示是，<img src="./pic/image-20230123224822796.png" alt="image-20230123224822796" style="zoom:67%;" />，其中<img src="./pic/image-20230123224858457.png" alt="image-20230123224858457" style="zoom:67%;" />是一个沿着时间轴的线性层。注意LTSF-Linear在不同变量之间共享权重，并且不建模任何空间相关性。

LTSF-Linear是一个线性模型的集合。Vanilla Linear是一个一层的线性模型。为了处理不同领域的时间序列（例如，金融，交通，能源领域），作者进一步引入了两个变种，有两个预处理过程，叫DLinear和NLinear。

- 具体来说，DLinear是Autoformer和FEDformer中使用的分解方案与线性层的组合。它首先通过移动平均kernel和季节分量将输入的原始数据分解为趋势分量。然后，将两个一层线性层应用于每个组件，并将这两个特征进行汇总以获得最终预测。通过显式处理趋势，当数据中有明确的趋势时，DLinear增强了普通线性的性能。
- 同时，当数据集中存在分布偏移时，为了提高LTSF Linear的性能，NLinear首先用序列的最后一个值减去输入。然后，输入经过一个线性层，减去的部分在进行最终预测之前加回来。NLinear中的减法和加法是输入序列的简单规范化。（其实就是差分操作）



### 5. 实验

#### 5.1. 实验设置

实验部分参考两篇博客。

**数据集。** 9个最广泛使用的真实世界的数据集。ETT（电力变压器温度，Electricity Transformer Temperature）(ETTh1, ETTh2, ETTm1,ETTm2), Traffic, Electricity, Weather, ILI, Exchange Rate [15]。这些数据集都是多元数间序列。在附录中，介绍了这些数据集。

**评估指标。** MSE和MAE。

**参与对比的方法。** FEDformer，Autoformer，Informer，Pyraformer，LogTrans。此外，还有一个朴素的DMS方法：Closest Repeat (*Repeat*)，它重复滑动窗口中的最后一个值，作为另一个简单的baseline。因为有两个FEDformer的变种，作者用准确率更好的那一个参与比较（经过傅里叶变换的FEDformer-f ）。

<img src="./pic/image-20230123230443459.png" alt="image-20230123230443459" style="zoom:50%;" />

#### 5.2. 和Transformer方法的比较

<img src="./pic/image-20230123230513471.png" alt="image-20230123230513471" style="zoom:50%;" />

<img src="./pic/image-20230125195947752.png" alt="image-20230125195947752" style="zoom:50%;" />

#### 5.3. 在LTSF-Transformer上的更多分析

**LTSF-Transformer能从更长的输入序列顺利提取时间关系吗？**滑动窗口的大小极大地影响了预测准确率，因为窗口大小决定了能从历史数据中学到多少东西。总之，一个好的具有很强的时间关系提取能力的TSF模型，窗口越大预测的结果应该越好。

结论：如果给定更长的序列，现有的解决方案倾向于过度拟合时间噪声，而不是提取时间信息，并且输入大小96正好适合大多数Transformer。

<img src="./pic/image-20230125200106549.png" alt="image-20230125200106549" style="zoom:50%;" />

**从长时期预测中可以学到什么？**虽然回看窗口中的时间动态显著影响短期时间序列预测的准确性，但我们假设长期预测取决于模型能否很好地捕捉趋势和周期。也就是说，预测范围越远，回看窗口本身的影响就越小。

<img src="./pic/image-20230125202804722.png" alt="image-20230125202804722" style="zoom:50%;" />



为了验证上述假设，在表3中，作者比较了两种输入（Far和Close）对准确率的影响。

结论：SOTA Transformer的性能略有下降，表明这些模型只能从相邻的时间序列中捕获相似的时间信息。由于捕获数据集的内在特征通常不需要大量的参数，即一个参数可以表示周期性。使用过多的参数甚至会导致过度拟合，这部分解释了为什么 LTSF-Linear方法的性能优于基于Transformer的方法。

**自注意力机制对LTSF有效吗？**我们验证现有Transformer（如Informer）中的这些复杂设计是否至关重要。在表4中，作者逐步的把Informer转换乘线性的。第一，把每层注意力层替换为线性层，叫Att.-Linear，因为自我注意层可以被看作是一个完全连通的层，其中权重是动态变化的。第二，作者丢弃了其他辅助设计（如FFN，即MLP），留下嵌入层和线性层，叫Embed+Linear。最后，把模型简化层一个线性层。令人惊讶的是，Informer的性能随着逐渐简化而增长，这表明至少对于现有的LTSF基准而言，自注意力机制和其他复杂模型是不必要的。

<img src="./pic/image-20230125203802793.png" alt="image-20230125203802793" style="zoom:50%;" />

**现有的LTSF-Transformer能否很好地保持时间顺序吗？**自注意力机制是不管顺序的。然而，在时间序列预测中，序列的顺序发挥了一个重要的作用。作者认为，即使有位置和时间嵌入，现有的基于Transformer的方法仍然受到时间信息丢失。在表5中，作者打乱了输入序列的顺序，在进入embed之前。用了两个打乱方式：*Shuf.*随机打乱整个输入序列，*Half-Ex*把前一半和后一半序列交换。有趣的是，在Exchange Rate数据集上，和步打乱顺序相比，基于Transformer的方法性能没有波动，甚至用完全随机打乱。相反，LTSF-Linea性能显著下降。

结论：这些结果表明，不同位置和时间嵌入的LTSF-Transformer保持相当有限的时间关系，并容易在噪声金融数据上过度拟合，而LTSF-Linear可以自然建模的顺序，避免过度拟合与较少的参数。

对于ETTh1数据集，FEDformer和Autoformer把时间序列的归纳偏置引入了模型，当数据集具有比Exchange Rate更清晰的时间模式（例如，周期性）时，使它们能够提取某些时间信息。因此这两个Transformer的性能平均下降了73.28%和56.91%，在随机打乱顺序的情况下。然而，Informer不管是*shuf.*和*Half-Ex.*都不怎么受影响，因为它没有引入归纳偏置。

结论：LTSF-Linear模型总体上比基于Transformer的模型性能下降更严重，表明，基于Transformer的方法不能很好的保留位置信息。

<img src="./pic/image-20230125205416768.png" alt="image-20230125205416768" style="zoom:50%;" />

**不同的嵌入策略有多有效？**作者研究了位置和时间戳嵌入的好处，在基于Transformer的方法中。在表6中，Informer的预测误差极大的提升了（没有位置嵌入）。没有时间戳嵌入将逐步地损害Informer的性能，随着预测长度的增加。因为Informer的每个token使用单个的时间步，因此有必要在token中引入时间信息。

FEDformer和Autoformer不是在每个token中使用单个时间步，而是输入一个时间戳序列来嵌入时间信息。因此，它们可以在没有固定位置嵌入的情况下实现类似甚至更好的性能。然而，如果没有时间戳嵌入，Autoformer的性能会迅速下降，这是全局时间信息丢失的原因。相反，由于在FEDformer中提出的频率增强模块引入了时间感应偏差，因此去除任何位置/时间戳嵌入对频率增强模块的影响较小。

<img src="./pic/image-20230125210659519.png" alt="image-20230125210659519" style="zoom:50%;" />

**训练集的大小是一个限制因素吗，对于LTSF-Transformer来说？**不像cv和nlp任务，TSF是在选定的时间序列上执行的，很难扩大训练数据的大小。事实上，训练集的大小确实对模型性能有显著的影响。因此，作者在Traffic数据集上进行了实验，比较了在整个数据集（17,544\*0.7 小时）上训练的模型（*Ori.*）和在缩短的数据集（8,760 小时，即1年）上训练的模型（*Short.*）。出乎意料的是，表7显示，在大多数情况下，训练数据减少的预测误差较低。这可能是因为全年数据比较长但不完整的数据保持了更清晰的时间特征。

结论：然而不能得出应该用更少的数据训练的结论，结果表明，训练数据规模不是影响Autoformer和FEDformer性能的限制因素。

<img src="./pic/image-20230125214619367.png" alt="image-20230125214619367" style="zoom:50%;" />

**效率真的是头等大事吗？**表8说明，原始的Transformer用在LSTF任务上，时空复杂度时可以接受的。

结论：不是很重要。

<img src="./pic/image-20230125214925996.png" alt="image-20230125214925996" style="zoom: 67%;" />

### 代码分析和遇到的问题

代码和PatchTST一模一样，只是把模型修改了。
