## Anomaly transformer: Time series anomaly detection with association discrepancy

作者：Jiehui Xu，Haixu Wu（清华大学）

来源： ICLR 2022

论文：[[arxiv](https://arxiv.org/pdf/2110.02642)]

代码：[[github](https://github.com/thuml/Anomaly-Transformer)]

引用数：45

参考：[[知乎1（作者博客）](https://zhuanlan.zhihu.com/p/466970544)]

关键词：Association Discrepancy，series-association，prior-association

### 摘要

异常点的无监督检测在时间序列中是一个具有挑战性的问题，这就要求模型推导出一个可区分的标准。以前的方法主要通过学习逐点表示或者逐对关联，然而，这两种方法都不足以对复杂的时间动态（dynamics）进行推理。最近，Transformer在逐点表示和逐对关联的统一建模方面展现出强大的能力，作者发现每个时间点的自注意权重分布可以体现与整个序列的丰富关联。我们的主要观测结果是，由于异常的罕见性，从异常点到整个序列建立非平凡关联是非常困难的，因此，异常的关联主要集中在相邻的时间点上。这种相邻浓度偏差意味着基于关联的标准在正常点和异常点之间本质上是可以区分的，我们通过关联差异来强调这一点（*Association Discrepancy*）。从技术上讲，我们提出了具有新的异常注意机制的Anomaly Transformer来计算关联差异。作者设计了极大极小策略来增强关联差异的正常-异常区分能力。Anomaly Transformer在服务监测、空间和地球探测以及水处理三个应用的六个无监督时间序列异常检测基准上取得了SOTA的结果。

### 1. 引言

现实世界的系统会有大量的连续数据，这些时序数据的异常检测对于保障系统安全、避免经济损失有着重要意义，例如服务器、地空设备的监测等。异常通常是罕见的，并且被大量的正常点隐藏起来，这使得数据标记变得困难和昂贵。本文关注**无监督时序异常检测**问题。

无监督时序异常检测在现实中有很大挑战性。模型通过无监督任务，从复杂的时间动态中学习信息表示。它也应该推导出一个可区分的标准，用来区分正常点和异常点。

无监督异常检测的方法（机器学习）：局部离群事实中提出的密度估计方法（LOF），基于聚类的单类支持向量机方法（OC-SVM和SVDD）。这些经典的方法没有考虑时间信息，并很难用到没见过的真实场景中。

无监督异常检测的方法（深度学习，RNN）：神经网络的表征学习能力，[Robust anomaly detection for 
multivariate time series through stochastic recurrent neural network(2019)]，[Timeseries anomaly detection using temporal hierarchical one-class network(2020)]，[Multivariate time series anomaly detection and interpretation using hierarchical inter-metric and temporal embedding(2021)]。这些性能很好。一大类方法侧重于通过精心设计的递归网络学习逐点表示，并通过重构或自回归任务进行自我监督。这里，一个自然和实用的异常标准是逐点重建或预测误差。然而，由于异常的罕见性，逐点表示对于复杂的时间模式来说信息量较小，并且可以被正常的时间点所主导，使得异常更难以区分。此外，重建或预测误差是逐点计算的，不能提供时间上下文的全面描述。

**因此，如何获取更具信息含量的表征，进而定义更加具有区分性的判据对于时序异常检测尤为关键。**

另一类主要的方法是基于显式关联建模来检测异常。向量自回归和状态空间模型属于这一类。该图还通过以不同时间点为顶点表示时间序列，并通过随机游走检测异常，来明确地描述关联[2008,2009]。一般来说，这些经典方法很难学习信息表示和建模细粒度关联。最近，图神经网络（GNN）被应用于学习多变量时间序列中多变量之间的动态图[Multivariate time-series anomaly detection via graph attention network(2020)]，[Graph neural network-based anomaly detection in multivariate time series(2021)]。虽然图神经网络的学到的表示更好，但是仍然局限于单个时间点，这对于复杂的时间模式来说是不够的。此外，基于子序列的方法通过计算子序列之间的相似性来检测异常[Graph-based subsequence anomaly detection for time series(2020)]。在探索更广泛的时间上下文时，这些方法无法捕捉每个时间点与整个序列之间的细粒度时间关联。

本篇论文中，作者使用了Transformer，在时序异常检测的无监督任务上。Transformer能看的更远，对全局信息进行统一建模。将Transformer应用于时间序列，发现可以从自注意图中得到各时间点的时间关联，~~其关联权重沿时间维分布~~。每个时间点的关联分布可以为时间上下文提供更多的信息描述，表明时间序列的周期或趋势等动态模式。我们将上面的关联分布命名为序列关联（*series-association*），它可以由 Transformers 从原始序列中发现。

此外，我们观察到，由于异常的罕见性和正常模式的主导性，异常很难与整个系列建立强有力的关联。异常的关联应集中在相邻的时间点，由于连续性，这些时间点更可能包含类似的异常模式。这种相邻的浓度归纳偏置被称为先验关联（*prior-association*）。相反，占主导地位正常时间点可以发现与整个系列的信息关联，而不限于相邻区域。基于这一观察，我们试图利用关联分布的固有正常-异常区分性。这导致了每个时间点的一个新的异常标准，通过每个时间点先前关联（*prior-association*）与其序列关联（*series-association*）之间的距离来量化，称为关联差异（*Association Discrepancy*）。如上所述，由于异常的关联更可能是集中在相邻的数据之间，异常将呈现比正常时间点**<font color=red>更小</font>**的关联差异。

超越以往的方法，我们将Transformer引入到无监督时间序列异常检测中，并提出了用于关联学习的Anomaly Transformer。为了计算关联差异（*Association Discrepancy*），作者把自注意力机制改成了异常注意力（*Anomaly-Attention*），它包含两个分支结构，分别对每个时间点的先验关联（*prior-association*）和序列关联（*series-association*）进行建模。先验关联使用可学习的高斯核来表示每个时间点的邻近集中（adjacent-
concentration）归纳偏置，而序列关联对应于从原始序列中学习的自我注意权重。此外，在两个分支之间应用了极大极小策略，该策略可以放大关联差异的正常-异常可区分性，并进一步导出一个新的基于关联的准则。Anomaly Transformer在六个基准测试上取得了很好的结果，涵盖了三个实际应用。贡献汇总如下：

- 基于对关联差异的关键观察，我们提出了具有异常注意机制的Anomaly Transformer，该Transformer可以同时对先验关联和序列关联进行建模，以体现关联差异。

- 我们提出了一种极大极小策略来放大关联差异的正常异常可区分性，并进一步导出了一种新的基于关联的检测准则。

- Anomaly Transformer在三个实际应用的六个基准上实现了最先进的异常检测结果。提供了广泛的消融和深入的案例研究。

### 2. 动机

不同于点级别表征，我们注意到在时间序列中每一个点都可以由其与整个序列的关联来表示，可以表示为其在**时间维度上的关联权重分布**。同时，相比于正常点来说，异常点很难与序列的所有点都构建很强的关联，且由于连续性往往更加关注邻近区域。这种**整个序列和邻近先验之间的关联差异**为时序异常检测提供了天然的、强区分度的判据。

基于上述观察，我们提出了Anomaly Transformer模型用于建模时序关联，同时利用极小极大（Minimax）关联学习策略进一步突出正常、异常点之间差别，实现了基于关联差异（Association Discrepancy）的时序异常检测。Anomaly Transformer在不同领域的5个数据集上都取得了SOTA的效果。
