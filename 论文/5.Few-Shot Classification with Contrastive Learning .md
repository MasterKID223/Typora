## Few-Shot Classification with Contrastive Learning



### 创新

在pre-training和meta-training中都加入了`instance-discriminative contrastive learning`

- pre-training

  加入了map-map和vector-map的自监督contrastive loss损失。

- meta-training

  将`contrastive learning`和正常的episodic训练结合。

  定义了`distance-scaled contrastive loss`，用来提升表示的传递性。



### 训练框架

![image-20221012192629433](./pic/image-20221012192629433.png)

其中，

```json
DA: '数据扩增'， // 把N个样本扩增到
CNN： '提取特征', //(C, H, W)
GAP: '全局特征', // (C), 输出h
Proj: '映射成D维的向量', // C -> D，一个隐层的MLP实现，输出z
Attn: '注意力机制', // 把number of heads修改为1
```



### pre-training

- 创新点

![image-20221012193630260](./pic/image-20221012193630260.png)



- 作用：

  self-supervision能提高表示的广泛性。

  使用supervised contrastive loss能捕捉相同类中不同实例之间的联系。

  

- 损失定义

  ![image-20221012201343575](./pic/image-20221012201343575.png)

  1. 

  ![image-20221012194305217](./pic/image-20221012194305217.png)

  其中，
  $$
  z_i:第一个view中Proj的输出，\\
  z_i^,: 第二个view中Proj的输出 \\
  \mathbb{1}:  indicator function
  $$
  

  2. 

     提高表示的鲁棒性。

     ![image-20221012195137095](./pic/image-20221012195137095.png)

     - **map-map**

       ![image-20221012195200344](./pic/image-20221012195200344.png)

       

       ![image-20221012195609629](./pic/image-20221012195609629.png)

       其中，

       $ f_q,f_k,f_v把输入的x_i分别映射成 $ query, key, value: $ q_i,k_i,v_i $

       ![image-20221012200036176](./pic/image-20221012200036176.png)

       ![image-20221012200049511](./pic/image-20221012200049511.png)

       两个view输入的x之间的相似度：

       ![image-20221012200230819](./pic/image-20221012200230819.png)

       维度：

       ![image-20221012200331401](./pic/image-20221012200331401.png)![image-20221012200356462](./pic/image-20221012200356462.png)

       

       

       

     - **vector-map** 

       ![image-20221012220903771](pic/image-20221012220903771.png)

       ![image-20221012195214030](./pic/image-20221012195214030.png)

       **FC的输出：**

       ![image-20221012201009433](./pic/image-20221012201009433.png)![image-20221012201027171](./pic/image-20221012201027171.png)

       ![image-20221012201105524](./pic/image-20221012201105524.png)

       

  3. 

     ![image-20221012201201187](./pic/image-20221012201201187.png)

     其中，

     ![image-20221012201219030](./pic/image-20221012201219030.png)

     $ P(i) $ 是除i外的所有索引的集合。



### Meta-training 

- 创新点

  `cross-viewepisodic training`机制

  ![image-20221012201532581](./pic/image-20221012201532581.png)

  在GAP之后计算prototype

  ![image-20221012202023412](./pic/image-20221012202023412.png)

  其中，

  $ S_r^k $是第r个view第k个类的support set，r = {1,2}，k = {1, 2, ..., M}，M是train set一共M个类。

  

  使用`Attn`去aligned prototype：

  ![image-20221012202334531](./pic/image-20221012202334531.png)

  基于上述结果计算query中样本的概率：
  ![image-20221012215236038](pic/image-20221012215236038.png)

  其中，d是欧式距离。

  

  然后一个episode上的` the nearest centroid classifier`损失：

  ![image-20221012215520859](pic/image-20221012215520859.png)

  其中，m,n = {1, 2}。

  不同于原始的episodic训练方法，这里在S1和S2上对Q1进行了分类，对Q2也执行了一样的操作。

  ![image-20221012215739718](pic/image-20221012215739718.png)

- Distance-scaled Contrastive Loss

  先计算经过一个`Proj`的prototype

  ![image-20221012221122257](pic/image-20221012221122257.png)

  > For each query vector zi in Q1 and Q2, we reconstruct its positive sample set by using corresponding augmented version z ′ i and the samples of class yi in both support sets.
  >
  > Therefore, we reformulate the supervised contrastive loss [19] in the form of episodic training as follows:
  
  ![image-20221012221213589](pic/image-20221012221213589.png)
  
  其中，
  
  ![image-20221012221505346](pic/image-20221012221505346.png)
  
  ![image-20221012221523859](pic/image-20221012221523859.png)
  
  然后，
  
  ![image-20221012221554141](pic/image-20221012221554141.png)
  
  ![image-20221012221605096](pic/image-20221012221605096.png)
  
  

​	
