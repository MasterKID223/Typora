## TIME SERIES GENERATION WITH MASKED AUTOENCODER

使用MAE产生时序数据

---



### 摘要

ExtraMAE(MAE with extrapolator)是一种可扩展的用于时序生成的自监督模型。E-MAE随即mask掉一些原时间序列，然后通过恢复这些masked patches学习时间动态(temporal dynamics)。本文的方法有两个核心设计。

**一、**E-MAE是自监督的。监督让E-MAE高效的捕捉原时间序的时间动态信息。

**二、** E-MAE提出了一个外推器(extrapolator)来分离解码器的两个工作：恢复潜在的表示并将其映射回特征空间。

这两个独特的设计让E-MAE在时序生成中的表现由于SoTA。这种轻量级的架构使E-MAE更快更易扩展。E-MAE在各种下游任务中展示了出色的结果，例如：时序分类、预测、补插imputation(?)。作为一个自监督生成模型，E-MAE允许对合成数据进行显式管理。作者希望本文将通过自我监督模型迎来时间序列生成的新时代。



### 2.方法

E-MAE从部分观测中恢复原始信号。E-MAE有三部分组成：encoder, extrapolator, decoder。encoder之作用于unmasked patches。一旦masked patches被编码，我们把unmasked patches的潜在表示外推到所有patches的潜在表示（masked and unmasked patches）。最后，decoder把完成的潜在表mask tokens示映射回特征空间。我门现在陈述问题，并介绍模型架构的细节。

- **Multivariate time series，多元时间序列**

  我们这样表示时间序列（time series）$ X = [x_1|x_2|...|x_L]\in\mathbb{R}^{d\times L} $  作为 $L$ 观测的一个序列。每个观测$ x_i = (x^1_i,x^2_i,...,x^d_i)^T\in\mathbb R^d $  由 $d$ 个特征组成，其中$ i = 1,2,...,L $.

- **Patches**

  我们把上述时间序列$ X \in \mathbb{R}^{d\times L} $ 切片成 $T$ 个规则的不重叠的patches $\{P_1,P_2,...,P_T\}$。第 $j$ 个patch $P_j\in\mathbb{R}^{d\times l} $ 由 $l = {L\over T}$ 个连续的观测组成，其中 $P_j = [x_{(j-1)l+1},x_{(j-1)l+2},...,x_{(j)l+1}]$ ，其中 $j = 1,2,...,T$。这里，$T$ 和 $l$ 都是 $L$ 的除数。在图 **1** 中，原时间序列由9条线组成，因此我们可以把时间序列分成 $T = 9$ 个patches。每个patch是一条线。

  ![image-20220924215356452](./pic/image-20220924215356452.png)

- **Masking**

  选 $m$ 个patches进行mask，剩下的不动($l-m$个)。masked patches的索引集合 $M = \{j_1,j_2,...j_m\} \subset \{1,2,...,T\}$。unmasked patches的索引集合 $N = \{1,2,...,T\}/M = \{k_1,k_2,...,k_n\}$ ，这里 $m + n = T$ 。为了简单起见，我们这样表示invisible masked patches $P_M = [P_{j1}|P_{j2}|...|P_{jm}]$ ，visible unmasked patches表示为 $P_N = [P_{k1}|P_{k2}|...|P_{kn}]$ 。注意在masked patch中的所有元素 $P_j \in \mathbb{R}^{d \times l} $ 将作为一个整体被移除，其中 $j \in M = \{j_1,j_2,...,j_m\}$。在图 **1 **中，我们随机maks $m = 4$ 个patches，同时，把剩下的 $n = 5$ 个patches合在一起得到 $P_N$ 。一条线一旦被masked掉就作为一个整体消失。masked patches的索引集合是 $M = \{3,5,7,9\}$。unmasked patches的索引集合 $N = \{1,2,4,6,8\}$。

  ![image-20220925111715349](./pic/image-20220925111715349.png)

- **Encoder**（维度的推导？）

  encoder只作用visible，unmasked patches $P_N \in \mathbb{R}^{d \times (n \cdot l)}$ 。encoder把unmasked patches $P_N \in \mathbb{R}^{d \times (n \cdot l)}$ 映射成它的潜在表示 $H_N \in \mathbb{R}^{h \times (n \cdot l)}$ 。这里 $h$ 是潜在空间(latent space)的维度。在图 **1** 中，encoder 把 $P_N = [P_1|P_2|P_4|P_6|P_8]$ 映射成了他们的潜在表示(整幅图左边的紫色方块)。我们把encoder表示成一个函数 $E : \prod_{k=1}^{n \cdot l}(\mathbb{R}^d)_k \to \prod_{k=1}^{n \cdot l}(\mathbb{R}^h)_k$ ，它的作用是把unmaksed patches $P_N$ 转化成他们的潜在表示 $H_N$ 。
  $$
  H_N = E(P_N)
  $$
  我们用一个**<u>堆叠的RNN</u>**(stacked RNN)实现函数 $E$ ，后面**<u>紧跟一个全连接层</u>**。注意encoder只作用于visible patches，跟masked tokens没关系。

  

- **Extrapolator**

  extrapolator恢复masked掉位置的潜在表示。我们把extrapolator表示成一个函数 $I : \prod_{k=1}^{n \times l}(\mathbb{R}^h)_k \to \prod_{k=1}^{T \times l}(\mathbb{R}^h)_k$ ，用来从visible邻居之间外推出遗失的(missing, masked)潜在表示。在图 **1** 中，extrapolator从unmasked patches的潜在表示 $H_N$ (左边紫色方块)推断出了所有patches的潜在表示 $\tilde{H}$ (右边的紫色方块)。我们用一个<u>**全连接层**</u>实现函数 $I$。
  $$
  \tilde{H} = I(H_N)
  $$
  ![image-20220925120801203](./pic/image-20220925120801203.png)

- **Decoder**

  decoder从extrapolator输出的潜在表示中重建原始信号。我们把decoder表示成 $D : \prod_{k=1}^{T \cdot l}(\mathbb{R}^h)_k \to \prod_{k=1}^{T \cdot l}(\mathbb{R}^d)_k$ ，它把潜在表示 $\tilde{H} \in \mathbb{R}^{h \times (T \cdot l)}$ 映射成合成的时间序列 $\hat{X} \in \mathbb{R}^{d \times (T \cdot l)}$ 。和encoder一样，$D$ 的实现是**<u>一个堆叠的RNN加一个全连接层</u>**。
  $$
  \hat{X} = D(\tilde{H})
  $$
  
- **Training** 

  首先，从原始的时间序列 $X$ 中随机mask一些patches。然后，E-MAE接收unmasked patches $P_N$ 并生成重建的时间序列 $\hat{X}$。我们的损失函数计算**<u>MSE损失</u>**，在重建的时间序列 $\hat{X}$ 和原始的时间序列 $X$ in $\mathbb{R}^{d \times (T \cdot l)}$。预计损失是：
  $$
  \ell_{recon} = \mathbb{E}_X||X - \hat{X}||_2
  $$
  **<u>不像MAE那样只计算masked patches上的损失</u>**，我们计算所有patches的损失。整体重建损失 $\ell_{recon}$ 确保重建的时间序列 $\hat{X}$ 保持了原始时间序 $X$ 的连续性。



- **Generation**

  E-MAE产生合成的数据*in a cross-validation like fashion*（图2）。我们把原始的时间序列 $X$ 分成了一些小褶皱（fold）。每次，E-MAE移除一个褶皱，然后根据部分观测值重建这个褶皱。我们重复这个过程，直到每个褶皱都被重建过。我们把这些重建过的褶皱组合到一起，就得到了最后的合成时间序列。惊奇的是，E-MAE在极端的mask比率下都能生成很好的时间序列。我们还尝试同时遮盖几个折叠。详见第3节和补充材料。
  
  ![image-20220925153911444](./pic/image-20220925153911444.png)

  图2：E-MAE的生成策略。此例中，原始的时间序列被分成4个folds。E-MAE每次mask一个fold，然后从部分时间观测中重建整个时间序列。我们保留合成的folds，然后组合在一起生成一个合成的时间序列。
  
- **Avoid mask tokens**

  在MAE中，一个decoder有两种作用：(i) 恢复missing position的信息。(ii) 把潜在的表示映射回特征空间。这个设计需要mask tokens。mask tokens是一种特殊的tokens，它指示了masked patches的存在。之前的研究中，decoders只接收不完整的潜在表示 $H_N \in \mathbb{R}^{h \times (n \cdot l)}$ 作为输入，因此需要mask tokens作为missing patches的指针。mask tokens在NLP模型中表现很好[17] [18]，但是在时序生成方面并不适合。与文本数据的高度离散的潜在空间相比，时序数据高度连续。mask tokens标记导致合成时间序列过于离散，无法保持原始时间序列的连续性（见补充资料）。因此，我们在E-MAE中避免使用mask tokens。E-MAE提出了一种外推法去恢复masked patches的潜在表示 $H_M \in \mathbb{R}^{h \times (m \cdot l)}$ 。然后decoder把完整的潜在表示$\tilde{H} \in \mathbb{R}^{h \times (T \cdot l)}$ 映射回了特征空间。因为decoder把完整的潜在表示$\tilde{H} \in \mathbb{R}^{h \times (T \cdot l)}$ 作为输入，我们就不再需要mask tokens了。我们在encoder中也避免了mask tokens，通过只操作visible unmasked patches $P_N$。

- **Statistical interpretation**

  MAE通过从部分观测中恢复原始信号来学习原始信号中的时间动态。统计插补[21]将此过程视为外推密度估计(extrapolation density estimation)。Extrapolation density $p(P_M|P_N)$ 是条件概率，其中 $P_M$ 是invisible masked patches，$P_N$ 是visible unmasked patches。因为extrapolation density包含了直接的时间相关性信息，E-MAE可以高效的捕捉到时间动态。extrapolation density的一个例子是预测。预测器在单调缺失的数据上估计extrapolation density。换句话说，预测将未知的未来归因于已知的过去。但是，单调的数据只是非单调数据的一个子集[21]。非单调缺失值上的外推密度比单调数据上的外推法密度更具信息性(informative)。这种场景下，E-MAE优于TimeGAN。

  在和其他benchmarks比较之前，我们提问：什么才是一个优秀的生成模型？对一个时序生成模型来说，有三个迫切的需求：（1）保真度(fidelity)：生成的合成数据应保持原始数据的时间动态。（2）实用性(practicality)：在实际应用中，生成的合成数据应该能够替代原始训练数据。（3）可操作性(maneuverability)：生成模型应该允许用户明确地管理合成数据。例如，用户可能希望为特定任务（例如预测、分类和插补）生成时间序列，或明确控制合成数据的保真度。本节显示，ExtraMAE在这三个方面都优于所有基准。

