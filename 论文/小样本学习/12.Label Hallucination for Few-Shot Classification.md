## Label Hallucination for Few-Shot Classification

作者：Yiren Jian（达特茅斯学院）

来源：AAAI 2022

论文：[[aaai](https://ojs.aaai.org/index.php/AAAI/article/view/20659/20418)]

代码：[[github](https://github.com/yiren-jian/LabelHalluc)]

引用数：10

### 创新点

如果在大型数据集上预训练模型然后微调用于小样本分类会导致严重的过拟合问题。与此同时，在从大型标记数据集中学习到的“冻结”特征上训练一个简单的线性分类器，不能使模型适应新类的特性，会导致欠拟合问题。

在本篇工作中，作者在novel数据集上训练了一个线性分类器，用这个分类器给base类里的图像打上“伪标签”，然后用蒸馏损失，在整个打了“伪标签”的base集合上微调模型。

打伪标签的目的：之前的一些工作都是在base类上预训练好一个特征提取器之后，冻结参数，在novel类中的每一个小样本分类任务上训练一个分类器，但是数据少，容易过拟合。所以，这篇的工作是不冻结参数，对整个体征提取器进行微调。那么微调过程中，数据还是很少，怎么办？这时就用到了“伪标签”，让模型在整个打上“伪标签”的base集合上微调，这样数据就够了。

<img src="pic/image-20221222030253613.png" alt="image-20221222030253613"  />

直觉上看，尽管novel类没有在base类中“正确”表示，但许多base图像可能包括与novel类相似的图像，这些图像由定义属于novel类概率的soft伪标签编码。

### 实验结果

| miniImageNet & tieredImageNet                               | CIFAR-FS & FC-100                                           |
| ----------------------------------------------------------- | ----------------------------------------------------------- |
| ![image-20221222030821330](pic/image-20221222030821330.png) | ![image-20221222030909924](pic/image-20221222030909924.png) |

| 不同的迁移方法                                              |
| ----------------------------------------------------------- |
| ![image-20221222031135381](pic/image-20221222031135381.png) |

### 3. 方法

#### 3.1. 在base数据集上训练特征提取器

用RFS中的训练方法训练：

![image-20221222031516224](pic/image-20221222031516224.png)

其中， $\Theta$ 是ResNet-12的参数， $\phi$ 是分类器（MLP）的参数。

根据之前的工作，还可以用自蒸馏，旋转自监督的方式进一步提升特征提取器的性能。

> In the experiements presented in this paper, we follow the embedding learning strategies of SKD (Ra-asegaran et al. 2020) (using self-supervised distillation) and IER (Rizve et al. 2021)  (leveraging invariant and equivariant representations).

#### 3.2. 用novel的类标签给base集合打“伪标签”

图1，每一个episode，（1）用support集合训练一个分类器， <img src="pic/image-20221222033022586.png" alt="image-20221222033022586" style="zoom: 67%;" /> （2）用这个分类器给整个base集合打上“伪标签”（分类器的softmax输出），（3)通过知识蒸馏重新训练整个网络。

#### 3.3. 微调整个模型识别novel类

接上面的第（3）步，用小批量，每个小批量support和base里样本的比例相同，微调整个网络。base样本用蒸馏损失，support样本用交叉熵损失。

![image-20221222033438345](pic/image-20221222033438345.png)

其中， $\hat{y}$ 是“伪标签”（softmax输出）， $\mathcal{L}_{KL}$ 是KL散度，KL散度的两项分别是：模型的输出，伪标签（softmax）。

由于support集合太小，用RFS中验证阶段的数据扩增方法增加support的view数量，达到小批量的一半，另一半是打“伪标签”的base样本。