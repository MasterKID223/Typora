## Rethinking Few-Shot Image Classification: A Good Embedding is All You Need?

来源：ECCV 2020

作者：Yonglong Tian（MIT）

被引数：489

下载：[[arxiv](https://arxiv.org/pdf/2003.11539)]

代码：[[github](http://github.com/WangYueFt/rfs/)]

### 摘要

在meta-training集上，学习一个监督或者自监督的表示，后面跟一个线性分类器。可以通过自蒸馏进一步提高性能。这说明了一个好的embed模块比复杂的meta-learning算法更有效。

### 介绍

本文的贡献：

- 一个非常简单的baseline，实现了SOT。
- 基于上述baseline，作者使用自蒸馏进一步改善性能。
- 不用监督训练，作者展示了用自监督学出来的特征，和监督方法学出来的特征性能差不多。

### 3. 方法

#### 3.1. 问题定义

- 基本的小样本问题
- 分类器 $\mathcal{A}$
- encoder $\Phi$

#### 3.2. 通过分类学到一个好的embed模块

- meta-training的目的是学到一个可迁移的embedding模块 $f_{\Phi}$，这个模块可以范化给任何新任务。

- 用分类任务，交叉熵损失训练了一个预训练模型，能产生很好的embedding特征。（传统大数据训练方法，把meta-training set的所有数据和在了一起）

  ![image-20221129234448582](./pic/image-20221129234448582.png)

- 如图2所示，以episode的方式，从meta-testing集中取了一个任务，用support训练一个LR，然后用LR对query分类。这个LR实现是一个多元逻辑回归。

  ![image-20221129234820304](./pic/image-20221129234820304.png)

- LR的训练方式
  $$
  \theta = \arg \min_{\{W, b\}} \sum\limits^T_{t=1} \mathcal{L}^{ce}_t (\mathbf{W}f_{\Phi}(\mathbf{X_t}) + \mathbf{b}, y_t) + \mathcal{R}(\mathbf{W, b})	\tag{6}
  $$
  

- 也可以使用NN，或者cosin相似度的方法分类（4.8节）

#### 3.3. 连续自蒸馏（Sequential Self-distillation）

[知识蒸馏](https://arxiv.org/pdf/1503.02531) 是一个方法用来把多模型集成里的embeded的知识迁移到单个模型里，或者从一个更大的老师模型迁移到更小的学生模型。

避免直接在meta-testing使用embedding模块，作者把知识从embedding模块蒸馏到有相同架构的新模型中，同样以大数据的训练方式（在合在一起的meta-training集中）。

新的embedding模块（参数是 $ \Phi^{'}$），用logits和label之间的交叉熵损失和predictions和soft targets之间的KL散度训练：
$$
\Phi^{'} = \arg \min_{\Phi^{'}} (\alpha \mathcal{L}^{ce}(\mathcal{D}^{new}; \Phi^{'}) + \beta KL(f(\mathcal{D}^{new}; \Phi^{'}), f(\mathcal{D}^{new}; \Phi^{'})))	\tag{7}
$$
其中，$\beta = 1- \alpha$

作者用Born-again[12]的策略，应用KD序列取生成了多代，如图3。在每一步，第k代的embedding模块都用第(k-1)代的迁移过来的知识训练：
$$
\Phi_k = \arg \min_{\Phi} (\alpha \mathcal{L}^{ce}(\mathcal{D}^{new}; \Phi) + \beta KL(f(\mathcal{D}^{new}; \Phi), f(\mathcal{D}^{new}; \Phi_{k-1})))	\tag{8}
$$
重复上述操作K次，最后的embedding模块就是 $\Phi_K$ 。

![image-20221130005609371](./pic/image-20221130005609371.png)