# Masked Autoencoders Are Scalable Vision Learners

Facebook AI Research (FAIR)

---



## 摘要

```tex
1. This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision.
	MAE 用于计算机视觉， masked autoencoders 是一种可扩展的自监督学习器。
2. MAE: 随机掩盖输入图像的像素，然后重建这些像素。
3. MAE 架构是一个不对称的的编码-解码架构，encoder 只对未遮掩的像素编码，轻量的 decoder 只对遮掩的像素和 latent representation(encoded patches) 重建。
4. 遮掩 75％ 以上的像素，产生一项重要自监督任务。
5. 综合上面的设计，能加速训练大型模型，加速3倍+。
6. (?) Transfer performance in downstream tasks outperforms supervised pretraining and shows promising scaling behavior.
```



## 方法

```tex
1. latent representation: encoder 对可见像素的映射(encoded patches)。
```

- `非对称架构`

  ```tex
  1. 预训练之后，decoder丢弃，encoder继续用于识别任务。
  ```

  > ![image-20220915215055140](F:\Typora\论文\Masked Autoencoders Are Scalable Vision Learners.assets\image-20220915215055140.png)

- `Masking`

  ```tex
  1. 规则、不重叠的对图像打补丁（patches）。
  2. 在 patches 中取一个子集，把剩余像素移除（mask）。
  3. 取子集的方法: 根据均匀分布不重复的随机取样。
  ```

  ```tex
  1. 对图像进行高比例的打补丁和移除像素，这样能够减少冗余，但是很难通过可见区域对图像进行修复（F2-4）。
  2. 均匀分布避免了潜在的中心偏移（所有补丁都在图像中间）。
  3. 高比例移除像素之后，输入就变成了一张高度稀疏的图，从而有利于后续高校编码器的设计。
  ```

  > Imagenet
  >
  > ![image-20220915222938132](F:\Typora\论文\Masked Autoencoders Are Scalable Vision Learners.assets\image-20220915222938132.png)
  >
  > COCO
  >
  > ![image-20220915223057255](F:\Typora\论文\Masked Autoencoders Are Scalable Vision Learners.assets\image-20220915223057255.png)
  >
  > ![image-20220915222827107](pic\image-20220915222827107.png)

- `MAE encoder`

  ```tex
  1. encoder是一种ViT，但只对未遮掩的像素编码。
  2. encoder通过一个线性映射（？）嵌入patch和位置embedding, 然后通过一系列的Transformer block处理结果集。
  3. encoder只处理25％左右的像素，masked patches不经过encoder，不适用mask tokens。
  ```

- `MAE decoder`

  ```tex
  1. decoder的输入是 visible patches 和 mask tokens。
  2. 每个mask token 都是一个共享的，可学习的向量，是待被修复 missing patch 的一种表示。
  3. 给所有的输入添加位置 embeddings，如果没有这个位置 embeddings，mask tokens 就不知道在图像中补到哪个位置。
  4. decoder 有和 encoder不一样的Transformer blocks。
  5. decoder 只在预训练过程中，重建图像。因此可以不考虑 encoder 独立设计 decoder。
  6. decoder 很小，有效的减少了预训练时间。	
  ```

- `Reconstruction target`

  ```tex
  1. 重建图像的原理：预测 mask patches 的像素值。
  2. decoder 输出的元素是表示 patch 的像素向量。
  3. decoder 的最后一层是一个 liner 映射，这个映射的通道数等于 patch 中像素值的数量（？）。
  4. decoder 的输出经 reshape 之后形成重建的图像。
  5. 损失函数计算了重建后的图像和原图像在像素空间上的 MSE 损失，只计算 masked patches 上的损失。
  
  6. 作者研究了一种变体，其重建目标是，每个 masked patch 的归一化像素值。
  7. 对于6.来说，计算一个 patch 中所有像素的均值和标准差，使用均值和标准差归一化（normalize） patch。
  8. 使用归一化的像素可以改善表示质量。
  ```

- `Simple implementation`

  ```tex
  1. 为每个输入的 patch 生成一个 token（通过一个带有位置 embedding 的线性映射）。
  2. 随机打乱（shuffle） token 列表的顺序，根据 masking ratio 移除 token 列表的最后部分。
  3. 这为 encoder 提供了一个 tokens 的子集。
  4. 在 encoding 之后，在 encoded patches 的列表后添加 mask tokens，对这个列表进行 unshuffle 操作，让所有的 tokens 和他们的 targets 对齐。
  5. decoder 应用于上述列表（带有位置 embeddings）。不需要稀疏操作。
  ```

## ImageNet上的实验

```tex
1. 在 ImageNet-1K 训练集上，做自监督预训练。
2. 然后做监督训练，评估端到端的 fine-tuning 或者 liner probing。
3。 在单个 224x224的剪裁上的得到了 top-1 的验证准确率。
```

- `Baseline:ViT-Large`

  ```tex
  1. 使用 ViT-Large 作为消融研究（ablation study）的 backbone。
  2. ViT-L 很大（比ResNet-50还大），并趋向于过拟合。
  3. 下面是一个比较：从0开始训练的ViT-L，作者在 baseline MAE 上的微调。(?)
  	说明了作者提出的 MAE 效果很好。
  ```

  > # ![image-20220916222610935](F:\Typora\论文\3.Masked Autoencoders Are Scalable Vision Learners.assets\image-20220916222610935.png)

## 4.1 Main Properties

```tex
1. 使用表1的默认配置消融（ablate） MAE。
```

> ![image-20220916223201718](F:\Typora\论文\3.Masked Autoencoders Are Scalable Vision Learners.assets\image-20220916223201718.png)

- `Masking ratio`

  ```tex
  1. masking ratio 的影响，和MAE的效果。
  2. 这里的 fine-tuning 和 linear probing 应该是一个并列的概念。
  3. 所有 fine-tuning 的结果都好于从0开始训练的结果。
  ```

  > ![image-20220916223401931](F:\Typora\论文\3.Masked Autoencoders Are Scalable Vision Learners.assets\image-20220916223401931.png)
  >
  > ![image-20220916223645838](F:\Typora\论文\3.Masked Autoencoders Are Scalable Vision Learners.assets\image-20220916223645838.png)

- `Decoder design`

  ```tex
  % Table 1(a), 1(b)
  1.  足够深的 decoder 对于 linear probing 很重要。可以根据 一个像素的重建和一个识别任务的 差距来解释：在一个 autoencoder 里，最后几层专门用来重建，和识别的关系不大。
  2. 如果使用 fine-tuning，最后几层可以用于识别任务，而 decoder 的深度对 fine-tuning 几乎没什么影响。
  3. 有趣的是，作者的MAE，只有单个block的decoder，在fine-tuning中表现得非常好。
  4. 单个得Transformer block是必须的，它生成从visible tokens到mask tokens的信息。只有单个block的decoder可以大大加速训练。
  
  5. 1(b)中研究了decoder维度，默认是512维，效果很好。
  6. （总结）MAE decoder有8个block，维度是512。
  ```

  > ![image-20220917090941596](F:\Typora\论文\3.Masked Autoencoders Are Scalable Vision Learners.assets\image-20220917090941596.png)

- `Mask token`

  ```tex
  % Table 1(c)
  1. MAE一个重要的设计就是跳过mask token(记作[M])，之后再把它应用到decoder上。
  % 接收mask tokens
  2. 如果encoder使用mask tokens，效果很差，在linear probling，准确率下降了14％。
  3. 在这种情况下，预训练模型和最后部署的模型会有区别：在预训练中，有大比例的mask tokens输入，但是这些mask tokens在原图(uncorrupted images)中不存在。这样的区别在部署后会降低准确率。
  4. 从encoder的输入中移除这些mask tokens，就限制了encoder只能看见真正的patches，从而改善准确率。
  5. 跳过mask tokens让计算速度快了3.3倍。
  ```

  > ![image-20220917094028433](F:\Typora\论文\3.Masked Autoencoders Are Scalable Vision Learners.assets\image-20220917094028433.png)

    ```tex
  % Table 2
  1. decoder 深度小，最后的准确率高。
    ```
  
  > ![image-20220917102115133](F:\Typora\论文\3.Masked Autoencoders Are Scalable Vision Learners.assets\image-20220917102115133.png)
